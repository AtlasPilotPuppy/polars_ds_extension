{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "from polars_ds.pipeline import Pipeline, Blueprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Builtin Pipeline Functions\n",
    "\n",
    "To run this demo: pip install polars_ds>=0.4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_parquet(\"../examples/dependency.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\n",
    "    pl.col(\"Loan_Period\").null_count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"POLARS_VERBOSE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars_ds as pds\n",
    "\n",
    "df.select(\n",
    "    pds.query_lstsq(\"Var1\", \"Existing_EMI\", target = \"Loan_Period\", skip_null=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blueprint first. \n",
    "# A blueprint is a plan for a pipeline. No hard work will be done until the blueprint is materialized, which\n",
    "# is when the tranforms are fitted (e.g. scale learns the mean and std from base data)\n",
    "# If target is specified for the blueprint, target will be excluded from all transformations that require a fit,\n",
    "# and target will be auto-filled if the transformation requires a target field and when no target field is explicitly given.\n",
    "\n",
    "bp = (\n",
    "    Blueprint(df, name = \"example\", target = \"approved\") # You can optionally put target of the ML model here\n",
    "    # Select only the columns we need\n",
    "    .lowercase() # lowercase all columns\n",
    "    .select(cs.numeric() | cs.by_name([\"gender\", \"employer_category1\", \"city_category\"]))\n",
    "    # explicitly put target, since this is not the target for prediction. \n",
    "    # Use a linear regression with x1 = var1, x2=existing_emi to predict missing values in loan_period\n",
    "    .linear_impute(features = [\"var1\", \"existing_emi\"], target = \"loan_period\") \n",
    "    .impute([\"existing_emi\"], method = \"median\")\n",
    "    .append_expr( # generate some features\n",
    "        pl.col(\"existing_emi\").log1p().alias(\"existing_emi_log1p\"),\n",
    "        pl.col(\"loan_amount\").log1p().alias(\"loan_amount_log1p\"),\n",
    "        pl.col(\"loan_amount\").sqrt().alias(\"loan_amount_sqrt\"),\n",
    "    )\n",
    "    .scale( # target is numerical, but will be excluded automatically because bp is initialzied with a target\n",
    "        cs.numeric().exclude([\"var1\", \"existing_emi_log1p\"]), method = \"standard\"\n",
    "    ) # Scale the columns up to this point. The columns below won't be scaled\n",
    "    .append_expr(\n",
    "        # Add missing flags\n",
    "        pl.col(\"employer_category1\").is_null().cast(pl.UInt8).alias(\"employer_category1_is_missing\")\n",
    "    )\n",
    "    .one_hot_encode(\"gender\", drop_first=True)\n",
    "    .woe_encode(\"city_category\") # No need to specify target because we initialized bp with a target\n",
    "    .target_encode(\"employer_category1\", min_samples_leaf = 20, smoothing = 10.0) # same as above\n",
    ")\n",
    "\n",
    "print(bp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Materialize the blueprint\n",
    "pipe:Pipeline = bp.materialize()\n",
    "# Text representation of the pipeline\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = pipe.transform(df)\n",
    "df_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialization Methods\n",
    "\n",
    "Pickle + JSON support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# The pipe object can be pickled\n",
    "with open(\"pipe.pickle\", \"wb\") as f:\n",
    "    pickle.dump(pipe, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pipe.pickle\", \"rb\") as f:\n",
    "    pipe2 = pickle.load(f)\n",
    "\n",
    "pipe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed_2 = pipe2.transform(df)\n",
    "df_transformed_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polars.testing import assert_frame_equal\n",
    "# True\n",
    "assert_frame_equal(df_transformed, df_transformed_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the pipeline as JSON\n",
    "\n",
    "pipe.to_json(\"test.json\")\n",
    "pipe3 = Pipeline.from_json(\"test.json\")\n",
    "# True\n",
    "assert_frame_equal(df_transformed, pipe3.transform(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Tranformations in Pipeline\n",
    "\n",
    "Need version >= v0.4.6 (Not released yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_parquet(\"../examples/dependency.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "\n",
    "# Any custom function must satistfy the following function signature:\n",
    "# func(df:Union[pl.DataFrame, pl.LazyFrame], cols: List[str], ...) -> List[pl.Expr]\n",
    "# where ... means kwargs\n",
    "# Here is a custom imputer\n",
    "\n",
    "def smallest_abs_impute(df:Union[pl.DataFrame, pl.LazyFrame], cols: List[str], epsilon:float = 0.01) -> List[pl.Expr]:\n",
    "    \"\"\"\n",
    "    Imputes columns by the min of the absolute values for c in columns, plus epsilon.\n",
    "    \"\"\"\n",
    "    temp = df.lazy().select(pl.col(cols).abs().min() + epsilon).collect().row(0)\n",
    "    return [pl.col(c).fill_null(m) for c, m in zip(cols, temp)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = (\n",
    "    Blueprint(df, name = \"example\", target = \"approved\")\n",
    "    .lowercase() # lowercase all columns\n",
    "    .append_fit_func(smallest_abs_impute, [\"var1\", \"existing_emi\", \"loan_amount\"], epsilon = 0.5)\n",
    "    # Use append_fit_func for custom transforms\n",
    ")\n",
    "# Notice that the value to impute is correct, it is 0.5, because the min abs of the columns are 0.\n",
    "pipe:Pipeline = bp.materialize()\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.transform(df).null_count().select([\"var1\", \"existing_emi\", \"loan_amount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
