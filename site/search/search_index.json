{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Polars-ds","text":"<p>A Polars Plugin aiming to simplify common numerical/string data analysis procedures. This means that the most basic data science, stats, NLP related tasks can be done natively inside a dataframe, without leaving dataframe world. This also means that for simple data pipelines, you do not need to install NumPy/Scipy/Scikit-learn, which saves a lot of space, which is great under constrained resources.</p> <p>Its goal is NOT to replace SciPy, or NumPy, but rather it tries reduce dependency for simple analysis, and tries to reduce Python side code and UDFs, which are often performance bottlenecks.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<pre><code>pip install polars_ds\n</code></pre> <p>and </p> <pre><code>import polars_ds\n</code></pre> <p>when you are using the namespaces provided by the package.</p>"},{"location":"#examples","title":"Examples","text":"<p>Generating random numbers, and running t-test, normality test inside a dataframe</p> <pre><code>df.with_columns(\n    pl.col(\"a\").stats_ext.sample_normal(mean = 0.5, std = 1.).alias(\"test1\")\n    , pl.col(\"a\").stats_ext.sample_normal(mean = 0.5, std = 2.).alias(\"test2\")\n).select(\n    pl.col(\"test1\").stats_ext.ttest_ind(pl.col(\"test2\"), equal_var = False).alias(\"t-test\")\n    , pl.col(\"test1\").stats_ext.normal_test().alias(\"normality_test\")\n).select(\n    pl.col(\"t-test\").struct.field(\"statistic\").alias(\"t-tests: statistics\")\n    , pl.col(\"t-test\").struct.field(\"pvalue\").alias(\"t-tests: pvalue\")\n    , pl.col(\"normality_test\").struct.field(\"statistic\").alias(\"normality_test: statistics\")\n    , pl.col(\"normality_test\").struct.field(\"pvalue\").alias(\"normality_test: pvalue\")\n)\n</code></pre> <p>Blazingly fast string similarity comparisons. (Thanks to RapidFuzz)</p> <pre><code>df2.select(\n    pl.col(\"word\").str_ext.levenshtein(\"world\", return_sim = True)\n).head()\n</code></pre> <p>And a lot more!</p>"},{"location":"complex_ext/","title":"Complex Extension","text":""},{"location":"complex_ext/#extension-for-complex-numbers","title":"Extension for Complex Numbers","text":""},{"location":"complex_ext/#polars_ds.complex_ext.ComplexExt","title":"<code>ComplexExt</code>","text":"<p>This class contains tools for dealing with complex numbers columns inside Polars dataframe.</p> <p>Polars Namespace: c</p> <p>Example: pl.col(\"a\").c.modulus()</p> <p>Complex number columns are represented as a column of size-2 lists. By default, an element will look like [re, im], which is in coordinate form. All operations (except powi, which turns it into polar form internally) assume the number is in coordinate form. There is a to_coord function provided for complex numbers in polar form [r, theta].</p> Source code in <code>python/polars_ds/complex_ext.py</code> <pre><code>@pl.api.register_expr_namespace(\"c\")\nclass ComplexExt:\n\n    \"\"\"\n    This class contains tools for dealing with complex numbers columns inside Polars dataframe.\n\n    Polars Namespace: c\n\n    Example: pl.col(\"a\").c.modulus()\n\n    Complex number columns are represented as a column of size-2 lists. By default, an element will look like [re, im],\n    which is in coordinate form. All operations (except powi, which turns it into polar form internally) assume the number\n    is in coordinate form. There is a to_coord function provided for complex numbers in polar form [r, theta].\n    \"\"\"\n\n    def __init__(self, expr: pl.Expr):\n        self._expr: pl.Expr = expr\n\n    def re(self) -&gt; pl.Expr:\n        \"\"\"Returns the real part of the complex number.\"\"\"\n        return self._expr.list.first()\n\n    def im(self) -&gt; pl.Expr:\n        \"\"\"Returns the imaginary part of the complex number.\"\"\"\n        return self._expr.list.last()\n\n    def to_complex(self) -&gt; pl.Expr:\n        \"\"\"Turns a column of floats into a column of complex with im = 0.\"\"\"\n        return pl.concat_list(self._expr, pl.lit(0.0, dtype=pl.Float64))\n\n    def with_imag(self, other: pl.Expr) -&gt; pl.Expr:\n        \"\"\"\n        Treats self as the real part, and other as the imaginary part and combines\n        them into a complex column. An alias for pl.concat_list(self._expr, other)\n        \"\"\"\n        return pl.concat_list(self._expr, other)\n\n    def modulus(self) -&gt; pl.Expr:\n        \"\"\"Returns the modulus of the complex number.\"\"\"\n        return self._expr.list.eval(pl.element().dot(pl.element()).sqrt()).list.first()\n\n    def squared_modulus(self) -&gt; pl.Expr:\n        \"\"\"Returns the squared modulus of the complex number.\"\"\"\n        return self._expr.list.eval(pl.element().dot(pl.element())).list.first()\n\n    def theta(self, degree: bool = False) -&gt; pl.Expr:\n        \"\"\"Returns the polar angle (in radians by default) of the complex number.\"\"\"\n        x = self._expr.list.first()\n        y = self._expr.list.last()\n        if degree:\n            return (\n                pl.when((x &gt; 0) | (y != 0))\n                .then(pl.arctan2d(y, x))\n                .when((x &lt; 0) &amp; (y == 0))\n                .then(pl.lit(180.0, dtype=pl.Float64))\n                .otherwise(pl.lit(math.nan, dtype=pl.Float64))\n            )\n        else:\n            return (\n                pl.when((x &gt; 0) | (y != 0))\n                .then(pl.arctan2(y, x))\n                .when((x &lt; 0) &amp; (y == 0))\n                .then(pl.lit(math.pi, dtype=pl.Float64))\n                .otherwise(pl.lit(math.nan, dtype=pl.Float64))\n            )\n\n    def to_polar(self) -&gt; pl.Expr:\n        \"\"\"Turns a complex number in coordinate form into polar form.\"\"\"\n        return pl.concat_list(self.modulus(), self.theta())\n\n    def to_coord(self) -&gt; pl.Expr:\n        \"\"\"Turns a complex number in polar form into coordinate form.\"\"\"\n        r = self._expr.list.first()\n        theta = self._expr.list.last()\n        return pl.concat_list(r * theta.cos(), r * theta.sin())\n\n    def conj(self) -&gt; pl.Expr:\n        \"\"\"Returns complex conjugate.\"\"\"\n        return pl.concat_list(self._expr.list.first(), -self._expr.list.last())\n\n    def add(self, other: Union[float, complex, pl.Expr]) -&gt; pl.Expr:\n        \"\"\"\n        Add either a single real, complex, or another col of complex to self. If other is\n        an expression, it must be another col of complex numbers.\n        \"\"\"\n        if isinstance(other, float):\n            return self._expr.list.eval(pl.element() + pl.Series([other, 0]))\n        if isinstance(other, complex):\n            return self._expr.list.eval(pl.element() + pl.Series([other.real, other.imag]))\n        else:\n            return pl.concat_list(\n                self._expr.list.first() + other.list.first(),\n                self._expr.list.last() + other.list.last(),\n            )\n\n    def sub(self, other: Union[float, complex, pl.Expr]) -&gt; pl.Expr:\n        \"\"\"\n        Subtract either a single real, complex, or another col of complex to self. If other is\n        an expression, it must be another col of complex numbers.\n        \"\"\"\n        if isinstance(other, float):\n            return self._expr.list.eval(pl.element() - pl.Series([other, 0]))\n        if isinstance(other, complex):\n            return self._expr.list.eval(pl.element() - pl.Series([other.real, other.imag]))\n        else:\n            return pl.concat_list(\n                self._expr.list.first() - other.list.first(),\n                self._expr.list.last() - other.list.last(),\n            )\n\n    def mul(self, other: Union[float, complex, pl.Expr]) -&gt; pl.Expr:\n        \"\"\"\n        Multiply either a single real, complex, or another col of complex to self. If other is\n        an expression, it must be another col of complex numbers.\n        \"\"\"\n        if isinstance(other, float):\n            return self._expr.list.eval(pl.element() * pl.lit(other))\n        if isinstance(other, complex):\n            x = self._expr.list.first()\n            y = self._expr.list.last()\n            new_real = x * other.real - y * other.imag\n            new_imag = x * other.imag + y * other.real\n            return pl.concat_list(new_real, new_imag)\n        else:\n            x = self._expr.list.first()\n            y = self._expr.list.last()\n            x2 = other.list.first()\n            y2 = other.list.last()\n            new_real = x * x2 - y * y2\n            new_imag = x * y2 + y * x2\n            return pl.concat_list(new_real, new_imag)\n\n    def inv(self) -&gt; pl.Expr:\n        \"\"\"Returns 1/z for a complex number z.\"\"\"\n        x = self._expr.list.first()\n        y = self._expr.list.last()\n        denom = x.pow(2) + y.pow(2)\n        return pl.concat_list(x / denom, -y / denom)\n\n    def div(self, other: Union[float, complex, pl.Expr]) -&gt; pl.Expr:\n        \"\"\"\n        Divide either a single real, complex, or another col of complex to self. If other is\n        an expression, it must be another col of complex numbers.\n        \"\"\"\n        if isinstance(other, float):\n            return self._expr.list.eval(pl.element() / pl.lit(other))\n        if isinstance(other, complex):\n            x = self._expr.list.first()\n            y = self._expr.list.last()\n            inverse = 1 / other\n            new_real = x * inverse.real - y * inverse.imag\n            new_imag = x * inverse.imag + y * inverse.real\n            return pl.concat_list(new_real, new_imag)\n        else:\n            x = self._expr.list.first()\n            y = self._expr.list.last()\n            x2 = other.list.first()\n            y2 = other.list.last()\n            denom = x2.pow(2) + y2.pow(2)\n            x_inv = x2 / denom\n            y_inv = -y2 / denom\n            new_real = x * x_inv - y * y_inv\n            new_imag = x * y_inv + y * x_inv\n            return pl.concat_list(new_real, new_imag)\n\n    def mul_by_i(self) -&gt; pl.Expr:\n        \"\"\"Multiplies self by i.\"\"\"\n        x = self._expr.list.first()\n        y = self._expr.list.last()\n        return pl.concat_list(-y, x)\n\n    def pow(self, x: float) -&gt; pl.Expr:\n        \"\"\"Raises a complex number to the x power.\"\"\"\n        if x == 0.0:\n            return pl.concat_list(\n                pl.when(self.modulus() == 0.0).then(math.nan).otherwise(1.0),\n                pl.lit(0.0, dtype=pl.Float64),\n            )\n        elif x == 1.0:\n            return self._expr\n        elif x == 2.0:\n            return self.mul(self._expr)\n        elif x == -1.0:\n            return self.inv()\n        else:\n            polar = self.to_polar()\n            r = polar.list.first()\n            theta = polar.list.last()\n            return pl.concat_list(r.pow(x) * (x * theta).cos(), r.pow(x) * (x * theta).sin())\n</code></pre>"},{"location":"complex_ext/#polars_ds.complex_ext.ComplexExt.add","title":"<code>add(other)</code>","text":"<p>Add either a single real, complex, or another col of complex to self. If other is an expression, it must be another col of complex numbers.</p> Source code in <code>python/polars_ds/complex_ext.py</code> <pre><code>def add(self, other: Union[float, complex, pl.Expr]) -&gt; pl.Expr:\n    \"\"\"\n    Add either a single real, complex, or another col of complex to self. If other is\n    an expression, it must be another col of complex numbers.\n    \"\"\"\n    if isinstance(other, float):\n        return self._expr.list.eval(pl.element() + pl.Series([other, 0]))\n    if isinstance(other, complex):\n        return self._expr.list.eval(pl.element() + pl.Series([other.real, other.imag]))\n    else:\n        return pl.concat_list(\n            self._expr.list.first() + other.list.first(),\n            self._expr.list.last() + other.list.last(),\n        )\n</code></pre>"},{"location":"complex_ext/#polars_ds.complex_ext.ComplexExt.conj","title":"<code>conj()</code>","text":"<p>Returns complex conjugate.</p> Source code in <code>python/polars_ds/complex_ext.py</code> <pre><code>def conj(self) -&gt; pl.Expr:\n    \"\"\"Returns complex conjugate.\"\"\"\n    return pl.concat_list(self._expr.list.first(), -self._expr.list.last())\n</code></pre>"},{"location":"complex_ext/#polars_ds.complex_ext.ComplexExt.div","title":"<code>div(other)</code>","text":"<p>Divide either a single real, complex, or another col of complex to self. If other is an expression, it must be another col of complex numbers.</p> Source code in <code>python/polars_ds/complex_ext.py</code> <pre><code>def div(self, other: Union[float, complex, pl.Expr]) -&gt; pl.Expr:\n    \"\"\"\n    Divide either a single real, complex, or another col of complex to self. If other is\n    an expression, it must be another col of complex numbers.\n    \"\"\"\n    if isinstance(other, float):\n        return self._expr.list.eval(pl.element() / pl.lit(other))\n    if isinstance(other, complex):\n        x = self._expr.list.first()\n        y = self._expr.list.last()\n        inverse = 1 / other\n        new_real = x * inverse.real - y * inverse.imag\n        new_imag = x * inverse.imag + y * inverse.real\n        return pl.concat_list(new_real, new_imag)\n    else:\n        x = self._expr.list.first()\n        y = self._expr.list.last()\n        x2 = other.list.first()\n        y2 = other.list.last()\n        denom = x2.pow(2) + y2.pow(2)\n        x_inv = x2 / denom\n        y_inv = -y2 / denom\n        new_real = x * x_inv - y * y_inv\n        new_imag = x * y_inv + y * x_inv\n        return pl.concat_list(new_real, new_imag)\n</code></pre>"},{"location":"complex_ext/#polars_ds.complex_ext.ComplexExt.im","title":"<code>im()</code>","text":"<p>Returns the imaginary part of the complex number.</p> Source code in <code>python/polars_ds/complex_ext.py</code> <pre><code>def im(self) -&gt; pl.Expr:\n    \"\"\"Returns the imaginary part of the complex number.\"\"\"\n    return self._expr.list.last()\n</code></pre>"},{"location":"complex_ext/#polars_ds.complex_ext.ComplexExt.inv","title":"<code>inv()</code>","text":"<p>Returns 1/z for a complex number z.</p> Source code in <code>python/polars_ds/complex_ext.py</code> <pre><code>def inv(self) -&gt; pl.Expr:\n    \"\"\"Returns 1/z for a complex number z.\"\"\"\n    x = self._expr.list.first()\n    y = self._expr.list.last()\n    denom = x.pow(2) + y.pow(2)\n    return pl.concat_list(x / denom, -y / denom)\n</code></pre>"},{"location":"complex_ext/#polars_ds.complex_ext.ComplexExt.modulus","title":"<code>modulus()</code>","text":"<p>Returns the modulus of the complex number.</p> Source code in <code>python/polars_ds/complex_ext.py</code> <pre><code>def modulus(self) -&gt; pl.Expr:\n    \"\"\"Returns the modulus of the complex number.\"\"\"\n    return self._expr.list.eval(pl.element().dot(pl.element()).sqrt()).list.first()\n</code></pre>"},{"location":"complex_ext/#polars_ds.complex_ext.ComplexExt.mul","title":"<code>mul(other)</code>","text":"<p>Multiply either a single real, complex, or another col of complex to self. If other is an expression, it must be another col of complex numbers.</p> Source code in <code>python/polars_ds/complex_ext.py</code> <pre><code>def mul(self, other: Union[float, complex, pl.Expr]) -&gt; pl.Expr:\n    \"\"\"\n    Multiply either a single real, complex, or another col of complex to self. If other is\n    an expression, it must be another col of complex numbers.\n    \"\"\"\n    if isinstance(other, float):\n        return self._expr.list.eval(pl.element() * pl.lit(other))\n    if isinstance(other, complex):\n        x = self._expr.list.first()\n        y = self._expr.list.last()\n        new_real = x * other.real - y * other.imag\n        new_imag = x * other.imag + y * other.real\n        return pl.concat_list(new_real, new_imag)\n    else:\n        x = self._expr.list.first()\n        y = self._expr.list.last()\n        x2 = other.list.first()\n        y2 = other.list.last()\n        new_real = x * x2 - y * y2\n        new_imag = x * y2 + y * x2\n        return pl.concat_list(new_real, new_imag)\n</code></pre>"},{"location":"complex_ext/#polars_ds.complex_ext.ComplexExt.mul_by_i","title":"<code>mul_by_i()</code>","text":"<p>Multiplies self by i.</p> Source code in <code>python/polars_ds/complex_ext.py</code> <pre><code>def mul_by_i(self) -&gt; pl.Expr:\n    \"\"\"Multiplies self by i.\"\"\"\n    x = self._expr.list.first()\n    y = self._expr.list.last()\n    return pl.concat_list(-y, x)\n</code></pre>"},{"location":"complex_ext/#polars_ds.complex_ext.ComplexExt.pow","title":"<code>pow(x)</code>","text":"<p>Raises a complex number to the x power.</p> Source code in <code>python/polars_ds/complex_ext.py</code> <pre><code>def pow(self, x: float) -&gt; pl.Expr:\n    \"\"\"Raises a complex number to the x power.\"\"\"\n    if x == 0.0:\n        return pl.concat_list(\n            pl.when(self.modulus() == 0.0).then(math.nan).otherwise(1.0),\n            pl.lit(0.0, dtype=pl.Float64),\n        )\n    elif x == 1.0:\n        return self._expr\n    elif x == 2.0:\n        return self.mul(self._expr)\n    elif x == -1.0:\n        return self.inv()\n    else:\n        polar = self.to_polar()\n        r = polar.list.first()\n        theta = polar.list.last()\n        return pl.concat_list(r.pow(x) * (x * theta).cos(), r.pow(x) * (x * theta).sin())\n</code></pre>"},{"location":"complex_ext/#polars_ds.complex_ext.ComplexExt.re","title":"<code>re()</code>","text":"<p>Returns the real part of the complex number.</p> Source code in <code>python/polars_ds/complex_ext.py</code> <pre><code>def re(self) -&gt; pl.Expr:\n    \"\"\"Returns the real part of the complex number.\"\"\"\n    return self._expr.list.first()\n</code></pre>"},{"location":"complex_ext/#polars_ds.complex_ext.ComplexExt.squared_modulus","title":"<code>squared_modulus()</code>","text":"<p>Returns the squared modulus of the complex number.</p> Source code in <code>python/polars_ds/complex_ext.py</code> <pre><code>def squared_modulus(self) -&gt; pl.Expr:\n    \"\"\"Returns the squared modulus of the complex number.\"\"\"\n    return self._expr.list.eval(pl.element().dot(pl.element())).list.first()\n</code></pre>"},{"location":"complex_ext/#polars_ds.complex_ext.ComplexExt.sub","title":"<code>sub(other)</code>","text":"<p>Subtract either a single real, complex, or another col of complex to self. If other is an expression, it must be another col of complex numbers.</p> Source code in <code>python/polars_ds/complex_ext.py</code> <pre><code>def sub(self, other: Union[float, complex, pl.Expr]) -&gt; pl.Expr:\n    \"\"\"\n    Subtract either a single real, complex, or another col of complex to self. If other is\n    an expression, it must be another col of complex numbers.\n    \"\"\"\n    if isinstance(other, float):\n        return self._expr.list.eval(pl.element() - pl.Series([other, 0]))\n    if isinstance(other, complex):\n        return self._expr.list.eval(pl.element() - pl.Series([other.real, other.imag]))\n    else:\n        return pl.concat_list(\n            self._expr.list.first() - other.list.first(),\n            self._expr.list.last() - other.list.last(),\n        )\n</code></pre>"},{"location":"complex_ext/#polars_ds.complex_ext.ComplexExt.theta","title":"<code>theta(degree=False)</code>","text":"<p>Returns the polar angle (in radians by default) of the complex number.</p> Source code in <code>python/polars_ds/complex_ext.py</code> <pre><code>def theta(self, degree: bool = False) -&gt; pl.Expr:\n    \"\"\"Returns the polar angle (in radians by default) of the complex number.\"\"\"\n    x = self._expr.list.first()\n    y = self._expr.list.last()\n    if degree:\n        return (\n            pl.when((x &gt; 0) | (y != 0))\n            .then(pl.arctan2d(y, x))\n            .when((x &lt; 0) &amp; (y == 0))\n            .then(pl.lit(180.0, dtype=pl.Float64))\n            .otherwise(pl.lit(math.nan, dtype=pl.Float64))\n        )\n    else:\n        return (\n            pl.when((x &gt; 0) | (y != 0))\n            .then(pl.arctan2(y, x))\n            .when((x &lt; 0) &amp; (y == 0))\n            .then(pl.lit(math.pi, dtype=pl.Float64))\n            .otherwise(pl.lit(math.nan, dtype=pl.Float64))\n        )\n</code></pre>"},{"location":"complex_ext/#polars_ds.complex_ext.ComplexExt.to_complex","title":"<code>to_complex()</code>","text":"<p>Turns a column of floats into a column of complex with im = 0.</p> Source code in <code>python/polars_ds/complex_ext.py</code> <pre><code>def to_complex(self) -&gt; pl.Expr:\n    \"\"\"Turns a column of floats into a column of complex with im = 0.\"\"\"\n    return pl.concat_list(self._expr, pl.lit(0.0, dtype=pl.Float64))\n</code></pre>"},{"location":"complex_ext/#polars_ds.complex_ext.ComplexExt.to_coord","title":"<code>to_coord()</code>","text":"<p>Turns a complex number in polar form into coordinate form.</p> Source code in <code>python/polars_ds/complex_ext.py</code> <pre><code>def to_coord(self) -&gt; pl.Expr:\n    \"\"\"Turns a complex number in polar form into coordinate form.\"\"\"\n    r = self._expr.list.first()\n    theta = self._expr.list.last()\n    return pl.concat_list(r * theta.cos(), r * theta.sin())\n</code></pre>"},{"location":"complex_ext/#polars_ds.complex_ext.ComplexExt.to_polar","title":"<code>to_polar()</code>","text":"<p>Turns a complex number in coordinate form into polar form.</p> Source code in <code>python/polars_ds/complex_ext.py</code> <pre><code>def to_polar(self) -&gt; pl.Expr:\n    \"\"\"Turns a complex number in coordinate form into polar form.\"\"\"\n    return pl.concat_list(self.modulus(), self.theta())\n</code></pre>"},{"location":"complex_ext/#polars_ds.complex_ext.ComplexExt.with_imag","title":"<code>with_imag(other)</code>","text":"<p>Treats self as the real part, and other as the imaginary part and combines them into a complex column. An alias for pl.concat_list(self._expr, other)</p> Source code in <code>python/polars_ds/complex_ext.py</code> <pre><code>def with_imag(self, other: pl.Expr) -&gt; pl.Expr:\n    \"\"\"\n    Treats self as the real part, and other as the imaginary part and combines\n    them into a complex column. An alias for pl.concat_list(self._expr, other)\n    \"\"\"\n    return pl.concat_list(self._expr, other)\n</code></pre>"},{"location":"num_ext/","title":"Numerical Extension","text":""},{"location":"num_ext/#extension-for-general-numerical-featuresmetricsquantities","title":"Extension for General Numerical Features/Metrics/Quantities","text":""},{"location":"num_ext/#polars_ds.num_ext.NumExt","title":"<code>NumExt</code>","text":"<p>This class contains tools for dealing with well-known numerical operations and other metrics inside Polars DataFrame.</p> <p>Polars Namespace: num_ext</p> <p>Example: pl.col(\"a\").num_ext.range_over_mean()</p> <p>It currently contains some time series stuff such as detrend, rfft, and time series metrics like SMAPE.</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>@pl.api.register_expr_namespace(\"num_ext\")\nclass NumExt:\n\n    \"\"\"\n    This class contains tools for dealing with well-known numerical operations and other metrics inside Polars DataFrame.\n\n    Polars Namespace: num_ext\n\n    Example: pl.col(\"a\").num_ext.range_over_mean()\n\n    It currently contains some time series stuff such as detrend, rfft, and time series metrics like SMAPE.\n    \"\"\"\n\n    def __init__(self, expr: pl.Expr):\n        self._expr: pl.Expr = expr\n\n    def binarize(self, cond: Optional[pl.Expr]) -&gt; pl.Expr:\n        \"\"\"\n        Binarize the column by a boolean condition.\n\n        Parameters\n        ----------\n        cond : Optional[pl.Expr]\n            If cond is none, this is equivalent to self._expr == self._expr.max(). If provided,\n            this will binarize by (self._expr &gt;= cond).\n        \"\"\"\n        if cond is None:\n            return (self._expr.eq(self._expr.max())).cast(pl.UInt8)\n        return (self._expr.ge(cond)).cast(pl.UInt8)\n\n    def std_err(self, ddof: int = 1) -&gt; pl.Expr:\n        \"\"\"\n        Estimates the standard error for the mean of the expression.\n        \"\"\"\n        return self._expr.std(ddof=ddof) / self._expr.count().sqrt()\n\n    def std_over_range(self, ddof: int = 1) -&gt; pl.Expr:\n        \"\"\"\n        Computes the standard deviation over the range.\n        \"\"\"\n        return self._expr.std(ddof=ddof) / (self._expr.max() - self._expr.min())\n\n    def rms(self) -&gt; pl.Expr:\n        \"\"\"\n        Returns root mean square of the expression\n        \"\"\"\n        return (self._expr.dot(self._expr) / self._expr.count()).sqrt()\n\n    def harmonic_mean(self) -&gt; pl.Expr:\n        \"\"\"\n        Returns the harmonic mean of the expression\n        \"\"\"\n        return self._expr.count() / (1.0 / self._expr).sum()\n\n    def geometric_mean(self) -&gt; pl.Expr:\n        \"\"\"\n        Returns the geometric mean of the expression\n        \"\"\"\n        return self._expr.product().pow(1.0 / self._expr.count())\n\n    def c_o_v(self, ddof: int = 1) -&gt; pl.Expr:\n        \"\"\"\n        Returns the coefficient of variation of the expression\n        \"\"\"\n        return self._expr.std(ddof=ddof) / self._expr.mean()\n\n    def range_over_mean(self) -&gt; pl.Expr:\n        \"\"\"\n        Returns (max - min) / mean\n        \"\"\"\n        return (self._expr.max() - self._expr.min()) / self._expr.mean()\n\n    def z_normalize(self, ddof: int = 1) -&gt; pl.Expr:\n        \"\"\"\n        z_normalize the given expression: remove the mean and scales by the std\n        \"\"\"\n        return (self._expr - self._expr.mean()) / self._expr.std(ddof=ddof)\n\n    def min_max_normalize(self) -&gt; pl.Expr:\n        \"\"\"\n        Min max normalize the given expression.\n        \"\"\"\n        return (self._expr - self._expr.min()) / (self._expr.max() - self._expr.min())\n\n    def frac(self) -&gt; pl.Expr:\n        \"\"\"\n        Returns the fractional part of the input values. E.g. fractional part of 1.1 is 0.1\n        \"\"\"\n        return self._expr.mod(1.0)\n\n    def max_abs(self) -&gt; pl.Expr:\n        \"\"\"\n        Returns the maximum of absolute values of self.\n        \"\"\"\n        return pl.max_horizontal(self._expr.max().abs(), self._expr.min().abs())\n\n    def n_bins(self, n: int) -&gt; pl.Expr:\n        \"\"\"\n        Maps values in this series into n bins, with each bin having equal size. This ensures that\n        the bins' ranges are the same, unlike quantiles. This may have tiny numerical errors but\n        should be tolerable.\n\n        Parameters\n        ----------\n        n\n            Any positive integer\n        \"\"\"\n        if n &lt;= 0:\n            raise ValueError(\"Input `n` must be positive.\")\n\n        x = self._expr\n        return (\n            (x - x.min()).floordiv(pl.lit(1e-12) + (x.max() - x.min()) / pl.lit(n)).cast(pl.UInt32)\n        )\n\n    def count_max(self) -&gt; pl.Expr:\n        \"\"\"\n        Count the number of occurrences of max.\n        \"\"\"\n        return (self._expr == self._expr.max()).sum()\n\n    def count_min(self) -&gt; pl.Expr:\n        \"\"\"\n        Count the number of occurrences of min.\n        \"\"\"\n        return (self._expr == self._expr.min()).sum()\n\n    def list_amax(self) -&gt; pl.Expr:\n        \"\"\"\n        Finds the argmax of the list in this column. This is useful for\n\n        (1) Turning sparse multiclass target into dense target.\n        (2) Finding the max probability class of a multiclass classification output.\n        (3) Just a shortcut for expr.list.eval(pl.element().arg_max()).\n        \"\"\"\n        return self._expr.list.eval(pl.element().arg_max())\n\n    def gcd(self, other: Union[int, pl.Expr]) -&gt; pl.Expr:\n        \"\"\"\n        Computes GCD of two integer columns. This will try to cast everything to int64.\n\n        Parameters\n        ----------\n        other\n            Either an int or a Polars expression\n        \"\"\"\n        if isinstance(other, int):\n            other_ = pl.lit(other, dtype=pl.Int64)\n        else:\n            other_ = other.cast(pl.Int64)\n\n        return self._expr.cast(pl.Int64).register_plugin(\n            lib=_lib,\n            symbol=\"pl_gcd\",\n            args=[other_],\n            is_elementwise=True,\n        )\n\n    def lcm(self, other: Union[int, pl.Expr]) -&gt; pl.Expr:\n        \"\"\"\n        Computes LCM of two integer columns. This will try to cast everything to int64.\n\n        Parameters\n        ----------\n        other\n            Either an int or a Polars expression\n        \"\"\"\n        if isinstance(other, int):\n            other_ = pl.lit(other, dtype=pl.Int64)\n        else:\n            other_ = other.cast(pl.Int64)\n\n        return self._expr.cast(pl.Int64).register_plugin(\n            lib=_lib,\n            symbol=\"pl_lcm\",\n            args=[other_],\n            is_elementwise=True,\n        )\n\n    def is_equidistant(self) -&gt; pl.Expr:\n        \"\"\"\n        Checks if a column has equal distance between consecutive values.\n        \"\"\"\n        return self._expr.diff(null_behavior=\"drop\").unique_counts().count().eq(1)\n\n    def hubor_loss(self, pred: pl.Expr, delta: float) -&gt; pl.Expr:\n        \"\"\"\n        Computes huber loss between this and the other expression. This assumes\n        this expression is actual, and the input is predicted, although the order\n        does not matter in this case.\n\n        Parameters\n        ----------\n        pred\n            An expression represeting the column with predicted probability.\n        \"\"\"\n        temp = (self._expr - pred).abs()\n        return (\n            pl.when(temp &lt;= delta).then(0.5 * temp.pow(2)).otherwise(delta * (temp - 0.5 * delta))\n            / self._expr.count()\n        )\n\n    def mad(self, pred: pl.Expr) -&gt; pl.Expr:\n        \"\"\"Computes mean absolute deivation between this and the other `pred` expression.\"\"\"\n        return (self._expr - pred).abs().mean()\n\n    def l1_loss(self, pred: pl.Expr, normalize: bool = True) -&gt; pl.Expr:\n        \"\"\"\n        Computes L1 loss (absolute difference) between this and the other `pred` expression.\n\n        Parameters\n        ----------\n        pred\n            An expression represeting the column with predicted probability.\n        normalize\n            If true, divide the result by length of the series\n        \"\"\"\n        temp = (self._expr - pred).abs().sum()\n        if normalize:\n            return temp / self._expr.count()\n        return temp\n\n    def l2_loss(self, pred: pl.Expr, normalize: bool = True) -&gt; pl.Expr:\n        \"\"\"\n        Computes L2 loss (normalized L2 distance) between this and the other `pred` expression. This\n        is the norm without 1/p power.\n\n        Parameters\n        ----------\n        pred\n            An expression represeting the column with predicted probability.\n        normalize\n            If true, divide the result by length of the series\n        \"\"\"\n        temp = self._expr - pred\n        temp = temp.dot(temp)\n        if normalize:\n            return temp / self._expr.count()\n        return temp\n\n    def msle(self, pred: pl.Expr, normalize: bool = True) -&gt; pl.Expr:\n        \"\"\"\n        Computes the mean square log error between this and the other `pred` expression.\n\n        Parameters\n        ----------\n        pred\n            An expression represeting the column with predicted probability.\n        normalize\n            If true, divide the result by length of the series\n        \"\"\"\n        diff = self._expr.log1p() - pred.log1p()\n        out = diff.dot(diff)\n        if normalize:\n            return out / self._expr.count()\n        return out\n\n    def chebyshev_loss(self, pred: pl.Expr, normalize: bool = True) -&gt; pl.Expr:\n        \"\"\"\n        Alias for l_inf_loss.\n        \"\"\"\n        return self.l_inf_dist(pred, normalize)\n\n    def l_inf_loss(self, pred: pl.Expr, normalize: bool = True) -&gt; pl.Expr:\n        \"\"\"\n        Computes L^infinity loss between this and the other `pred` expression\n\n        Parameters\n        ----------\n        pred\n            An expression represeting the column with predicted probability.\n        normalize\n            If true, divide the result by length of the series\n        \"\"\"\n        temp = self._expr - pred\n        out = pl.max_horizontal(temp.min().abs(), temp.max().abs())\n        if normalize:\n            return out / self._expr.count()\n        return out\n\n    def mape(self, pred: pl.Expr, weighted: bool = False) -&gt; pl.Expr:\n        \"\"\"\n        Computes mean absolute percentage error between self and the other `pred` expression.\n        If weighted, it will compute the weighted version as defined here:\n\n        https://en.wikipedia.org/wiki/Mean_absolute_percentage_error\n\n        Parameters\n        ----------\n        pred\n            An expression represeting the column with predicted probability.\n        weighted\n            If true, computes wMAPE in the wikipedia article\n        \"\"\"\n        if weighted:\n            return (self._expr - pred).abs().sum() / self._expr.abs().sum()\n        else:\n            return (1 - pred / self._expr).abs().mean()\n\n    def smape(self, pred: pl.Expr) -&gt; pl.Expr:\n        \"\"\"\n        Computes symmetric mean absolute percentage error between self and other `pred` expression.\n        The value is always between 0 and 1. This is the third version in the wikipedia without\n        the 100 factor.\n\n        https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error\n\n        Parameters\n        ----------\n        pred\n            A Polars expression representing predictions\n        \"\"\"\n        numerator = (self._expr - pred).abs()\n        denominator = 1.0 / (self._expr.abs() + pred.abs())\n        return (1.0 / self._expr.count()) * numerator.dot(denominator)\n\n    def logloss(self, pred: pl.Expr, normalize: bool = True) -&gt; pl.Expr:\n        \"\"\"\n        Computes log loss, aka binary cross entropy loss, between self and other `pred` expression.\n\n        Parameters\n        ----------\n        pred\n            An expression represeting the column with predicted probability.\n        normalize\n            Whether to divide by N.\n        \"\"\"\n        out = self._expr.dot(pred.log()) + (1 - self._expr).dot((1 - pred).log())\n        if normalize:\n            return -(out / self._expr.count())\n        return -out\n\n    def bce(self, pred: pl.Expr, normalize: bool = True) -&gt; pl.Expr:\n        \"\"\"\n        Binary cross entropy. Alias for logloss.\n        \"\"\"\n        return self.logloss(pred, normalize)\n\n    def roc_auc(self, pred: pl.Expr) -&gt; pl.Expr:\n        \"\"\"\n        Computes ROC AUC using self as actual and pred as predictions.\n\n        Self must be binary and castable to type UInt32. If self is not all 0s and 1s or not binary,\n        the result will not make sense, or some error may occur.\n\n        Parameters\n        ----------\n        pred\n            An expression represeting the column with predicted probability.\n        \"\"\"\n        y = self._expr.cast(pl.UInt32)\n        return y.register_plugin(\n            lib=_lib,\n            symbol=\"pl_roc_auc\",\n            args=[pred],\n            is_elementwise=False,\n            returns_scalar=True,\n        )\n\n    def binary_metrics_combo(self, pred: pl.Expr, threshold: float = 0.5) -&gt; pl.Expr:\n        \"\"\"\n        Computes the following binary classificaition metrics using self as actual and pred as predictions:\n        precision, recall, f, average_precision and roc_auc. The return will be a struct with values\n        having the names as given here.\n\n        Self must be binary and castable to type UInt32. If self is not all 0s and 1s,\n        the result will not make sense, or some error may occur.\n\n        Average precision is computed using Sum (R_n - R_n-1)*P_n-1, which is not the textbook definition,\n        but is consistent with Scikit-learn. For more information, see\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html\n\n        Parameters\n        ----------\n        pred\n            An expression represeting the column with predicted probability.\n        threshold\n            The threshold used to compute precision, recall and f (f score).\n        \"\"\"\n        y = self._expr.cast(pl.UInt32)\n        return y.register_plugin(\n            lib=_lib,\n            symbol=\"pl_combo_b\",\n            args=[pred, pl.lit(threshold, dtype=pl.Float64)],\n            is_elementwise=False,\n            returns_scalar=True,\n        )\n\n    def trapz(self, x: Union[float, pl.Expr]) -&gt; pl.Expr:\n        \"\"\"\n        Treats self as y axis, integrates along x using the trapezoidal rule. If x is not a single\n        value, then x should be sorted.\n\n        Parameters\n        ----------\n        x\n            If it is a single float, it must be positive and it will represent a uniform\n            distance between points. If it is an expression, it must be sorted, does not contain\n            null, and have the same length as self.\n        \"\"\"\n        y = self._expr.cast(pl.Float64)\n        if isinstance(x, float):\n            x_ = pl.lit(abs(x), pl.Float64)\n        else:\n            x_ = x.cast(pl.Float64)\n\n        return y.register_plugin(\n            lib=_lib,\n            symbol=\"pl_trapz\",\n            args=[x_],\n            is_elementwise=False,\n            returns_scalar=True,\n        )\n\n    def r2(self, pred: pl.Expr) -&gt; pl.Expr:\n        \"\"\"\n        Returns the coefficient of determineation for a regression model.\n\n        Parameters\n        ----------\n        pred\n            A Polars expression representing predictions\n        \"\"\"\n        diff = self._expr - pred\n        ss_res = diff.dot(diff)\n        diff2 = self._expr - self._expr.mean()\n        ss_tot = diff2.dot(diff2)\n        return 1.0 - ss_res / ss_tot\n\n    def adjusted_r2(self, pred: pl.Expr, p: int) -&gt; pl.Expr:\n        \"\"\"\n        Returns the adjusted r2 for a regression model.\n\n        Parameters\n        ----------\n        pred\n            A Polars expression representing predictions\n        p\n            The total number of explanatory variables in the model\n        \"\"\"\n        diff = self._expr - pred\n        ss_res = diff.dot(diff)\n        diff2 = self._expr - self._expr.mean()\n        ss_tot = diff2.dot(diff2)\n        df_res = self._expr.count() - p\n        df_tot = self._expr.count() - 1\n        return 1.0 - (ss_res / df_res) / (ss_tot / df_tot)\n\n    # Not recommended to use\n    def powi(self, n: Union[int, pl.Expr]) -&gt; pl.Expr:\n        \"\"\"\n        Computes positive integer power using the fast exponentiation algorithm. This is the\n        fastest when n is an integer input (Faster than Polars's builtin when n &gt;= 16). When n\n        is an expression, it would depend on values in the expression (Still researching...)\n\n        Parameters\n        ----------\n        n\n            A single positive int or an expression representing a column of type i32. If type is\n            not i32, an error will occur.\n        \"\"\"\n\n        if isinstance(n, int):\n            n_ = pl.lit(n, pl.Int32)\n        else:\n            n_ = n\n\n        return self._expr.register_plugin(\n            lib=_lib, symbol=\"pl_fast_exp\", args=[n_], is_elementwise=True, returns_scalar=False\n        )\n\n    def jaccard(self, other: pl.Expr, include_null: bool = False) -&gt; pl.Expr:\n        \"\"\"\n        Computes jaccard similarity between this column and the other. This will hash entire\n        columns and compares the two hashsets. Note: only integer/str columns can be compared.\n        Input expressions must represent columns of the same dtype.\n\n        Parameters\n        ----------\n        other\n            Either an int or a Polars expression\n        include_null\n            Whether to include null as a distinct element.\n        \"\"\"\n        return self._expr.register_plugin(\n            lib=_lib,\n            symbol=\"pl_jaccard\",\n            args=[other, pl.lit(include_null, dtype=pl.Boolean)],\n            is_elementwise=False,\n            returns_scalar=True,\n        )\n\n    def list_jaccard(self, other: pl.Expr) -&gt; pl.Expr:\n        \"\"\"\n        Computes jaccard similarity pairwise between this and the other column. The type of\n        each column must be list and the lists must have the same inner type. The inner type\n        must either be integer or string.\n\n        Parameters\n        ----------\n        other\n            Either an int or a Polars expression\n        include_null : to be added\n            Currently there are some technical issue with adding this parameter.\n        \"\"\"\n        return self._expr.register_plugin(\n            lib=_lib,\n            symbol=\"pl_list_jaccard\",\n            args=[other],\n            is_elementwise=True,\n        )\n\n    def cond_entropy(self, other: pl.Expr) -&gt; pl.Expr:\n        \"\"\"\n        Computes the conditional entropy of self(y) given other, aka. H(y|other).\n\n        Parameters\n        ----------\n        other\n            A Polars expression\n        \"\"\"\n\n        return self._expr.register_plugin(\n            lib=_lib,\n            symbol=\"pl_conditional_entropy\",\n            args=[other],\n            is_elementwise=False,\n            returns_scalar=True,\n        )\n\n    def lstsq(self, *others: pl.Expr, add_bias: bool = False) -&gt; pl.Expr:\n        \"\"\"\n        Computes least squares solution to the equation Ax = y. If columns are\n        not linearly independent, some numerical issue may occur. E.g you may see\n        unrealistic coefficient in the output. It is possible to have `silent` numerical\n        issue during computation.\n\n        All positional arguments should be expressions representing predictive variables. This\n        does not support composite expressions like pl.col([\"a\", \"b\"]), pl.all(), etc.\n\n        If add_bias is true, it will be the last coefficient in the output\n        and output will have len(others) + 1\n\n        Parameters\n        ----------\n        others\n            Polars expressions.\n        add_bias\n            Whether to add a bias term\n        \"\"\"\n        y = self._expr.cast(pl.Float64)\n        return y.register_plugin(\n            lib=_lib,\n            symbol=\"pl_lstsq\",\n            args=[pl.lit(add_bias, dtype=pl.Boolean)] + list(others),\n            is_elementwise=False,\n            returns_scalar=True,\n        )\n\n    def detrend(self, method: DetrendMethod = \"linear\") -&gt; pl.Expr:\n        \"\"\"\n        Detrends self using either linear/mean method.\n\n        Parameters\n        ----------\n        method\n            Either `linear` or `mean`\n        \"\"\"\n\n        if method == \"linear\":\n            N = self._expr.count()\n            x = pl.int_range(0, N, dtype=pl.Float64, eager=False)\n            coeff = pl.cov(self._expr, x) / x.var()\n            const = self._expr.mean() - coeff * (N - 1) / 2\n            return self._expr - x * coeff - const\n        elif method == \"mean\":\n            return self._expr - self._expr.mean()\n        else:\n            raise ValueError(f\"Unknown detrend method: {method}\")\n\n    def rfft(self, length: Optional[int] = None) -&gt; pl.Expr:\n        \"\"\"\n        Computes the DFT transform of a real-valued input series using FFT Algorithm. Note that\n        a series of length (length // 2 + 1) will be returned.\n\n        Parameters\n        ----------\n        length\n            A positive integer\n        \"\"\"\n        # Add a k step argument?\n        if length is not None and length &lt;= 1:\n            raise ValueError(\"Input `length` should be &gt; 1.\")\n\n        le = pl.lit(length, dtype=pl.UInt32)\n        x: pl.Expr = self._expr.cast(pl.Float64)\n        return x.register_plugin(\n            lib=_lib, symbol=\"pl_rfft\", args=[le], is_elementwise=False, changes_length=True\n        )\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.adjusted_r2","title":"<code>adjusted_r2(pred, p)</code>","text":"<p>Returns the adjusted r2 for a regression model.</p>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.adjusted_r2--parameters","title":"Parameters","text":"<p>pred     A Polars expression representing predictions p     The total number of explanatory variables in the model</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def adjusted_r2(self, pred: pl.Expr, p: int) -&gt; pl.Expr:\n    \"\"\"\n    Returns the adjusted r2 for a regression model.\n\n    Parameters\n    ----------\n    pred\n        A Polars expression representing predictions\n    p\n        The total number of explanatory variables in the model\n    \"\"\"\n    diff = self._expr - pred\n    ss_res = diff.dot(diff)\n    diff2 = self._expr - self._expr.mean()\n    ss_tot = diff2.dot(diff2)\n    df_res = self._expr.count() - p\n    df_tot = self._expr.count() - 1\n    return 1.0 - (ss_res / df_res) / (ss_tot / df_tot)\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.bce","title":"<code>bce(pred, normalize=True)</code>","text":"<p>Binary cross entropy. Alias for logloss.</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def bce(self, pred: pl.Expr, normalize: bool = True) -&gt; pl.Expr:\n    \"\"\"\n    Binary cross entropy. Alias for logloss.\n    \"\"\"\n    return self.logloss(pred, normalize)\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.binarize","title":"<code>binarize(cond)</code>","text":"<p>Binarize the column by a boolean condition.</p>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.binarize--parameters","title":"Parameters","text":"<p>cond : Optional[pl.Expr]     If cond is none, this is equivalent to self._expr == self._expr.max(). If provided,     this will binarize by (self._expr &gt;= cond).</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def binarize(self, cond: Optional[pl.Expr]) -&gt; pl.Expr:\n    \"\"\"\n    Binarize the column by a boolean condition.\n\n    Parameters\n    ----------\n    cond : Optional[pl.Expr]\n        If cond is none, this is equivalent to self._expr == self._expr.max(). If provided,\n        this will binarize by (self._expr &gt;= cond).\n    \"\"\"\n    if cond is None:\n        return (self._expr.eq(self._expr.max())).cast(pl.UInt8)\n    return (self._expr.ge(cond)).cast(pl.UInt8)\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.binary_metrics_combo","title":"<code>binary_metrics_combo(pred, threshold=0.5)</code>","text":"<p>Computes the following binary classificaition metrics using self as actual and pred as predictions: precision, recall, f, average_precision and roc_auc. The return will be a struct with values having the names as given here.</p> <p>Self must be binary and castable to type UInt32. If self is not all 0s and 1s, the result will not make sense, or some error may occur.</p> <p>Average precision is computed using Sum (R_n - R_n-1)*P_n-1, which is not the textbook definition, but is consistent with Scikit-learn. For more information, see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html</p>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.binary_metrics_combo--parameters","title":"Parameters","text":"<p>pred     An expression represeting the column with predicted probability. threshold     The threshold used to compute precision, recall and f (f score).</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def binary_metrics_combo(self, pred: pl.Expr, threshold: float = 0.5) -&gt; pl.Expr:\n    \"\"\"\n    Computes the following binary classificaition metrics using self as actual and pred as predictions:\n    precision, recall, f, average_precision and roc_auc. The return will be a struct with values\n    having the names as given here.\n\n    Self must be binary and castable to type UInt32. If self is not all 0s and 1s,\n    the result will not make sense, or some error may occur.\n\n    Average precision is computed using Sum (R_n - R_n-1)*P_n-1, which is not the textbook definition,\n    but is consistent with Scikit-learn. For more information, see\n    https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html\n\n    Parameters\n    ----------\n    pred\n        An expression represeting the column with predicted probability.\n    threshold\n        The threshold used to compute precision, recall and f (f score).\n    \"\"\"\n    y = self._expr.cast(pl.UInt32)\n    return y.register_plugin(\n        lib=_lib,\n        symbol=\"pl_combo_b\",\n        args=[pred, pl.lit(threshold, dtype=pl.Float64)],\n        is_elementwise=False,\n        returns_scalar=True,\n    )\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.c_o_v","title":"<code>c_o_v(ddof=1)</code>","text":"<p>Returns the coefficient of variation of the expression</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def c_o_v(self, ddof: int = 1) -&gt; pl.Expr:\n    \"\"\"\n    Returns the coefficient of variation of the expression\n    \"\"\"\n    return self._expr.std(ddof=ddof) / self._expr.mean()\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.chebyshev_loss","title":"<code>chebyshev_loss(pred, normalize=True)</code>","text":"<p>Alias for l_inf_loss.</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def chebyshev_loss(self, pred: pl.Expr, normalize: bool = True) -&gt; pl.Expr:\n    \"\"\"\n    Alias for l_inf_loss.\n    \"\"\"\n    return self.l_inf_dist(pred, normalize)\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.cond_entropy","title":"<code>cond_entropy(other)</code>","text":"<p>Computes the conditional entropy of self(y) given other, aka. H(y|other).</p>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.cond_entropy--parameters","title":"Parameters","text":"<p>other     A Polars expression</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def cond_entropy(self, other: pl.Expr) -&gt; pl.Expr:\n    \"\"\"\n    Computes the conditional entropy of self(y) given other, aka. H(y|other).\n\n    Parameters\n    ----------\n    other\n        A Polars expression\n    \"\"\"\n\n    return self._expr.register_plugin(\n        lib=_lib,\n        symbol=\"pl_conditional_entropy\",\n        args=[other],\n        is_elementwise=False,\n        returns_scalar=True,\n    )\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.count_max","title":"<code>count_max()</code>","text":"<p>Count the number of occurrences of max.</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def count_max(self) -&gt; pl.Expr:\n    \"\"\"\n    Count the number of occurrences of max.\n    \"\"\"\n    return (self._expr == self._expr.max()).sum()\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.count_min","title":"<code>count_min()</code>","text":"<p>Count the number of occurrences of min.</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def count_min(self) -&gt; pl.Expr:\n    \"\"\"\n    Count the number of occurrences of min.\n    \"\"\"\n    return (self._expr == self._expr.min()).sum()\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.detrend","title":"<code>detrend(method='linear')</code>","text":"<p>Detrends self using either linear/mean method.</p>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.detrend--parameters","title":"Parameters","text":"<p>method     Either <code>linear</code> or <code>mean</code></p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def detrend(self, method: DetrendMethod = \"linear\") -&gt; pl.Expr:\n    \"\"\"\n    Detrends self using either linear/mean method.\n\n    Parameters\n    ----------\n    method\n        Either `linear` or `mean`\n    \"\"\"\n\n    if method == \"linear\":\n        N = self._expr.count()\n        x = pl.int_range(0, N, dtype=pl.Float64, eager=False)\n        coeff = pl.cov(self._expr, x) / x.var()\n        const = self._expr.mean() - coeff * (N - 1) / 2\n        return self._expr - x * coeff - const\n    elif method == \"mean\":\n        return self._expr - self._expr.mean()\n    else:\n        raise ValueError(f\"Unknown detrend method: {method}\")\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.frac","title":"<code>frac()</code>","text":"<p>Returns the fractional part of the input values. E.g. fractional part of 1.1 is 0.1</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def frac(self) -&gt; pl.Expr:\n    \"\"\"\n    Returns the fractional part of the input values. E.g. fractional part of 1.1 is 0.1\n    \"\"\"\n    return self._expr.mod(1.0)\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.gcd","title":"<code>gcd(other)</code>","text":"<p>Computes GCD of two integer columns. This will try to cast everything to int64.</p>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.gcd--parameters","title":"Parameters","text":"<p>other     Either an int or a Polars expression</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def gcd(self, other: Union[int, pl.Expr]) -&gt; pl.Expr:\n    \"\"\"\n    Computes GCD of two integer columns. This will try to cast everything to int64.\n\n    Parameters\n    ----------\n    other\n        Either an int or a Polars expression\n    \"\"\"\n    if isinstance(other, int):\n        other_ = pl.lit(other, dtype=pl.Int64)\n    else:\n        other_ = other.cast(pl.Int64)\n\n    return self._expr.cast(pl.Int64).register_plugin(\n        lib=_lib,\n        symbol=\"pl_gcd\",\n        args=[other_],\n        is_elementwise=True,\n    )\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.geometric_mean","title":"<code>geometric_mean()</code>","text":"<p>Returns the geometric mean of the expression</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def geometric_mean(self) -&gt; pl.Expr:\n    \"\"\"\n    Returns the geometric mean of the expression\n    \"\"\"\n    return self._expr.product().pow(1.0 / self._expr.count())\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.harmonic_mean","title":"<code>harmonic_mean()</code>","text":"<p>Returns the harmonic mean of the expression</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def harmonic_mean(self) -&gt; pl.Expr:\n    \"\"\"\n    Returns the harmonic mean of the expression\n    \"\"\"\n    return self._expr.count() / (1.0 / self._expr).sum()\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.hubor_loss","title":"<code>hubor_loss(pred, delta)</code>","text":"<p>Computes huber loss between this and the other expression. This assumes this expression is actual, and the input is predicted, although the order does not matter in this case.</p>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.hubor_loss--parameters","title":"Parameters","text":"<p>pred     An expression represeting the column with predicted probability.</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def hubor_loss(self, pred: pl.Expr, delta: float) -&gt; pl.Expr:\n    \"\"\"\n    Computes huber loss between this and the other expression. This assumes\n    this expression is actual, and the input is predicted, although the order\n    does not matter in this case.\n\n    Parameters\n    ----------\n    pred\n        An expression represeting the column with predicted probability.\n    \"\"\"\n    temp = (self._expr - pred).abs()\n    return (\n        pl.when(temp &lt;= delta).then(0.5 * temp.pow(2)).otherwise(delta * (temp - 0.5 * delta))\n        / self._expr.count()\n    )\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.is_equidistant","title":"<code>is_equidistant()</code>","text":"<p>Checks if a column has equal distance between consecutive values.</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def is_equidistant(self) -&gt; pl.Expr:\n    \"\"\"\n    Checks if a column has equal distance between consecutive values.\n    \"\"\"\n    return self._expr.diff(null_behavior=\"drop\").unique_counts().count().eq(1)\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.jaccard","title":"<code>jaccard(other, include_null=False)</code>","text":"<p>Computes jaccard similarity between this column and the other. This will hash entire columns and compares the two hashsets. Note: only integer/str columns can be compared. Input expressions must represent columns of the same dtype.</p>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.jaccard--parameters","title":"Parameters","text":"<p>other     Either an int or a Polars expression include_null     Whether to include null as a distinct element.</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def jaccard(self, other: pl.Expr, include_null: bool = False) -&gt; pl.Expr:\n    \"\"\"\n    Computes jaccard similarity between this column and the other. This will hash entire\n    columns and compares the two hashsets. Note: only integer/str columns can be compared.\n    Input expressions must represent columns of the same dtype.\n\n    Parameters\n    ----------\n    other\n        Either an int or a Polars expression\n    include_null\n        Whether to include null as a distinct element.\n    \"\"\"\n    return self._expr.register_plugin(\n        lib=_lib,\n        symbol=\"pl_jaccard\",\n        args=[other, pl.lit(include_null, dtype=pl.Boolean)],\n        is_elementwise=False,\n        returns_scalar=True,\n    )\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.l1_loss","title":"<code>l1_loss(pred, normalize=True)</code>","text":"<p>Computes L1 loss (absolute difference) between this and the other <code>pred</code> expression.</p>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.l1_loss--parameters","title":"Parameters","text":"<p>pred     An expression represeting the column with predicted probability. normalize     If true, divide the result by length of the series</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def l1_loss(self, pred: pl.Expr, normalize: bool = True) -&gt; pl.Expr:\n    \"\"\"\n    Computes L1 loss (absolute difference) between this and the other `pred` expression.\n\n    Parameters\n    ----------\n    pred\n        An expression represeting the column with predicted probability.\n    normalize\n        If true, divide the result by length of the series\n    \"\"\"\n    temp = (self._expr - pred).abs().sum()\n    if normalize:\n        return temp / self._expr.count()\n    return temp\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.l2_loss","title":"<code>l2_loss(pred, normalize=True)</code>","text":"<p>Computes L2 loss (normalized L2 distance) between this and the other <code>pred</code> expression. This is the norm without 1/p power.</p>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.l2_loss--parameters","title":"Parameters","text":"<p>pred     An expression represeting the column with predicted probability. normalize     If true, divide the result by length of the series</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def l2_loss(self, pred: pl.Expr, normalize: bool = True) -&gt; pl.Expr:\n    \"\"\"\n    Computes L2 loss (normalized L2 distance) between this and the other `pred` expression. This\n    is the norm without 1/p power.\n\n    Parameters\n    ----------\n    pred\n        An expression represeting the column with predicted probability.\n    normalize\n        If true, divide the result by length of the series\n    \"\"\"\n    temp = self._expr - pred\n    temp = temp.dot(temp)\n    if normalize:\n        return temp / self._expr.count()\n    return temp\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.l_inf_loss","title":"<code>l_inf_loss(pred, normalize=True)</code>","text":"<p>Computes L^infinity loss between this and the other <code>pred</code> expression</p>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.l_inf_loss--parameters","title":"Parameters","text":"<p>pred     An expression represeting the column with predicted probability. normalize     If true, divide the result by length of the series</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def l_inf_loss(self, pred: pl.Expr, normalize: bool = True) -&gt; pl.Expr:\n    \"\"\"\n    Computes L^infinity loss between this and the other `pred` expression\n\n    Parameters\n    ----------\n    pred\n        An expression represeting the column with predicted probability.\n    normalize\n        If true, divide the result by length of the series\n    \"\"\"\n    temp = self._expr - pred\n    out = pl.max_horizontal(temp.min().abs(), temp.max().abs())\n    if normalize:\n        return out / self._expr.count()\n    return out\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.lcm","title":"<code>lcm(other)</code>","text":"<p>Computes LCM of two integer columns. This will try to cast everything to int64.</p>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.lcm--parameters","title":"Parameters","text":"<p>other     Either an int or a Polars expression</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def lcm(self, other: Union[int, pl.Expr]) -&gt; pl.Expr:\n    \"\"\"\n    Computes LCM of two integer columns. This will try to cast everything to int64.\n\n    Parameters\n    ----------\n    other\n        Either an int or a Polars expression\n    \"\"\"\n    if isinstance(other, int):\n        other_ = pl.lit(other, dtype=pl.Int64)\n    else:\n        other_ = other.cast(pl.Int64)\n\n    return self._expr.cast(pl.Int64).register_plugin(\n        lib=_lib,\n        symbol=\"pl_lcm\",\n        args=[other_],\n        is_elementwise=True,\n    )\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.list_amax","title":"<code>list_amax()</code>","text":"<p>Finds the argmax of the list in this column. This is useful for</p> <p>(1) Turning sparse multiclass target into dense target. (2) Finding the max probability class of a multiclass classification output. (3) Just a shortcut for expr.list.eval(pl.element().arg_max()).</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def list_amax(self) -&gt; pl.Expr:\n    \"\"\"\n    Finds the argmax of the list in this column. This is useful for\n\n    (1) Turning sparse multiclass target into dense target.\n    (2) Finding the max probability class of a multiclass classification output.\n    (3) Just a shortcut for expr.list.eval(pl.element().arg_max()).\n    \"\"\"\n    return self._expr.list.eval(pl.element().arg_max())\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.list_jaccard","title":"<code>list_jaccard(other)</code>","text":"<p>Computes jaccard similarity pairwise between this and the other column. The type of each column must be list and the lists must have the same inner type. The inner type must either be integer or string.</p>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.list_jaccard--parameters","title":"Parameters","text":"<p>other     Either an int or a Polars expression include_null : to be added     Currently there are some technical issue with adding this parameter.</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def list_jaccard(self, other: pl.Expr) -&gt; pl.Expr:\n    \"\"\"\n    Computes jaccard similarity pairwise between this and the other column. The type of\n    each column must be list and the lists must have the same inner type. The inner type\n    must either be integer or string.\n\n    Parameters\n    ----------\n    other\n        Either an int or a Polars expression\n    include_null : to be added\n        Currently there are some technical issue with adding this parameter.\n    \"\"\"\n    return self._expr.register_plugin(\n        lib=_lib,\n        symbol=\"pl_list_jaccard\",\n        args=[other],\n        is_elementwise=True,\n    )\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.logloss","title":"<code>logloss(pred, normalize=True)</code>","text":"<p>Computes log loss, aka binary cross entropy loss, between self and other <code>pred</code> expression.</p>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.logloss--parameters","title":"Parameters","text":"<p>pred     An expression represeting the column with predicted probability. normalize     Whether to divide by N.</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def logloss(self, pred: pl.Expr, normalize: bool = True) -&gt; pl.Expr:\n    \"\"\"\n    Computes log loss, aka binary cross entropy loss, between self and other `pred` expression.\n\n    Parameters\n    ----------\n    pred\n        An expression represeting the column with predicted probability.\n    normalize\n        Whether to divide by N.\n    \"\"\"\n    out = self._expr.dot(pred.log()) + (1 - self._expr).dot((1 - pred).log())\n    if normalize:\n        return -(out / self._expr.count())\n    return -out\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.lstsq","title":"<code>lstsq(*others, add_bias=False)</code>","text":"<p>Computes least squares solution to the equation Ax = y. If columns are not linearly independent, some numerical issue may occur. E.g you may see unrealistic coefficient in the output. It is possible to have <code>silent</code> numerical issue during computation.</p> <p>All positional arguments should be expressions representing predictive variables. This does not support composite expressions like pl.col([\"a\", \"b\"]), pl.all(), etc.</p> <p>If add_bias is true, it will be the last coefficient in the output and output will have len(others) + 1</p>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.lstsq--parameters","title":"Parameters","text":"<p>others     Polars expressions. add_bias     Whether to add a bias term</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def lstsq(self, *others: pl.Expr, add_bias: bool = False) -&gt; pl.Expr:\n    \"\"\"\n    Computes least squares solution to the equation Ax = y. If columns are\n    not linearly independent, some numerical issue may occur. E.g you may see\n    unrealistic coefficient in the output. It is possible to have `silent` numerical\n    issue during computation.\n\n    All positional arguments should be expressions representing predictive variables. This\n    does not support composite expressions like pl.col([\"a\", \"b\"]), pl.all(), etc.\n\n    If add_bias is true, it will be the last coefficient in the output\n    and output will have len(others) + 1\n\n    Parameters\n    ----------\n    others\n        Polars expressions.\n    add_bias\n        Whether to add a bias term\n    \"\"\"\n    y = self._expr.cast(pl.Float64)\n    return y.register_plugin(\n        lib=_lib,\n        symbol=\"pl_lstsq\",\n        args=[pl.lit(add_bias, dtype=pl.Boolean)] + list(others),\n        is_elementwise=False,\n        returns_scalar=True,\n    )\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.mad","title":"<code>mad(pred)</code>","text":"<p>Computes mean absolute deivation between this and the other <code>pred</code> expression.</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def mad(self, pred: pl.Expr) -&gt; pl.Expr:\n    \"\"\"Computes mean absolute deivation between this and the other `pred` expression.\"\"\"\n    return (self._expr - pred).abs().mean()\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.mape","title":"<code>mape(pred, weighted=False)</code>","text":"<p>Computes mean absolute percentage error between self and the other <code>pred</code> expression. If weighted, it will compute the weighted version as defined here:</p> <p>https://en.wikipedia.org/wiki/Mean_absolute_percentage_error</p>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.mape--parameters","title":"Parameters","text":"<p>pred     An expression represeting the column with predicted probability. weighted     If true, computes wMAPE in the wikipedia article</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def mape(self, pred: pl.Expr, weighted: bool = False) -&gt; pl.Expr:\n    \"\"\"\n    Computes mean absolute percentage error between self and the other `pred` expression.\n    If weighted, it will compute the weighted version as defined here:\n\n    https://en.wikipedia.org/wiki/Mean_absolute_percentage_error\n\n    Parameters\n    ----------\n    pred\n        An expression represeting the column with predicted probability.\n    weighted\n        If true, computes wMAPE in the wikipedia article\n    \"\"\"\n    if weighted:\n        return (self._expr - pred).abs().sum() / self._expr.abs().sum()\n    else:\n        return (1 - pred / self._expr).abs().mean()\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.max_abs","title":"<code>max_abs()</code>","text":"<p>Returns the maximum of absolute values of self.</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def max_abs(self) -&gt; pl.Expr:\n    \"\"\"\n    Returns the maximum of absolute values of self.\n    \"\"\"\n    return pl.max_horizontal(self._expr.max().abs(), self._expr.min().abs())\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.min_max_normalize","title":"<code>min_max_normalize()</code>","text":"<p>Min max normalize the given expression.</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def min_max_normalize(self) -&gt; pl.Expr:\n    \"\"\"\n    Min max normalize the given expression.\n    \"\"\"\n    return (self._expr - self._expr.min()) / (self._expr.max() - self._expr.min())\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.msle","title":"<code>msle(pred, normalize=True)</code>","text":"<p>Computes the mean square log error between this and the other <code>pred</code> expression.</p>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.msle--parameters","title":"Parameters","text":"<p>pred     An expression represeting the column with predicted probability. normalize     If true, divide the result by length of the series</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def msle(self, pred: pl.Expr, normalize: bool = True) -&gt; pl.Expr:\n    \"\"\"\n    Computes the mean square log error between this and the other `pred` expression.\n\n    Parameters\n    ----------\n    pred\n        An expression represeting the column with predicted probability.\n    normalize\n        If true, divide the result by length of the series\n    \"\"\"\n    diff = self._expr.log1p() - pred.log1p()\n    out = diff.dot(diff)\n    if normalize:\n        return out / self._expr.count()\n    return out\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.n_bins","title":"<code>n_bins(n)</code>","text":"<p>Maps values in this series into n bins, with each bin having equal size. This ensures that the bins' ranges are the same, unlike quantiles. This may have tiny numerical errors but should be tolerable.</p>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.n_bins--parameters","title":"Parameters","text":"<p>n     Any positive integer</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def n_bins(self, n: int) -&gt; pl.Expr:\n    \"\"\"\n    Maps values in this series into n bins, with each bin having equal size. This ensures that\n    the bins' ranges are the same, unlike quantiles. This may have tiny numerical errors but\n    should be tolerable.\n\n    Parameters\n    ----------\n    n\n        Any positive integer\n    \"\"\"\n    if n &lt;= 0:\n        raise ValueError(\"Input `n` must be positive.\")\n\n    x = self._expr\n    return (\n        (x - x.min()).floordiv(pl.lit(1e-12) + (x.max() - x.min()) / pl.lit(n)).cast(pl.UInt32)\n    )\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.powi","title":"<code>powi(n)</code>","text":"<p>Computes positive integer power using the fast exponentiation algorithm. This is the fastest when n is an integer input (Faster than Polars's builtin when n &gt;= 16). When n is an expression, it would depend on values in the expression (Still researching...)</p>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.powi--parameters","title":"Parameters","text":"<p>n     A single positive int or an expression representing a column of type i32. If type is     not i32, an error will occur.</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def powi(self, n: Union[int, pl.Expr]) -&gt; pl.Expr:\n    \"\"\"\n    Computes positive integer power using the fast exponentiation algorithm. This is the\n    fastest when n is an integer input (Faster than Polars's builtin when n &gt;= 16). When n\n    is an expression, it would depend on values in the expression (Still researching...)\n\n    Parameters\n    ----------\n    n\n        A single positive int or an expression representing a column of type i32. If type is\n        not i32, an error will occur.\n    \"\"\"\n\n    if isinstance(n, int):\n        n_ = pl.lit(n, pl.Int32)\n    else:\n        n_ = n\n\n    return self._expr.register_plugin(\n        lib=_lib, symbol=\"pl_fast_exp\", args=[n_], is_elementwise=True, returns_scalar=False\n    )\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.r2","title":"<code>r2(pred)</code>","text":"<p>Returns the coefficient of determineation for a regression model.</p>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.r2--parameters","title":"Parameters","text":"<p>pred     A Polars expression representing predictions</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def r2(self, pred: pl.Expr) -&gt; pl.Expr:\n    \"\"\"\n    Returns the coefficient of determineation for a regression model.\n\n    Parameters\n    ----------\n    pred\n        A Polars expression representing predictions\n    \"\"\"\n    diff = self._expr - pred\n    ss_res = diff.dot(diff)\n    diff2 = self._expr - self._expr.mean()\n    ss_tot = diff2.dot(diff2)\n    return 1.0 - ss_res / ss_tot\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.range_over_mean","title":"<code>range_over_mean()</code>","text":"<p>Returns (max - min) / mean</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def range_over_mean(self) -&gt; pl.Expr:\n    \"\"\"\n    Returns (max - min) / mean\n    \"\"\"\n    return (self._expr.max() - self._expr.min()) / self._expr.mean()\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.rfft","title":"<code>rfft(length=None)</code>","text":"<p>Computes the DFT transform of a real-valued input series using FFT Algorithm. Note that a series of length (length // 2 + 1) will be returned.</p>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.rfft--parameters","title":"Parameters","text":"<p>length     A positive integer</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def rfft(self, length: Optional[int] = None) -&gt; pl.Expr:\n    \"\"\"\n    Computes the DFT transform of a real-valued input series using FFT Algorithm. Note that\n    a series of length (length // 2 + 1) will be returned.\n\n    Parameters\n    ----------\n    length\n        A positive integer\n    \"\"\"\n    # Add a k step argument?\n    if length is not None and length &lt;= 1:\n        raise ValueError(\"Input `length` should be &gt; 1.\")\n\n    le = pl.lit(length, dtype=pl.UInt32)\n    x: pl.Expr = self._expr.cast(pl.Float64)\n    return x.register_plugin(\n        lib=_lib, symbol=\"pl_rfft\", args=[le], is_elementwise=False, changes_length=True\n    )\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.rms","title":"<code>rms()</code>","text":"<p>Returns root mean square of the expression</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def rms(self) -&gt; pl.Expr:\n    \"\"\"\n    Returns root mean square of the expression\n    \"\"\"\n    return (self._expr.dot(self._expr) / self._expr.count()).sqrt()\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.roc_auc","title":"<code>roc_auc(pred)</code>","text":"<p>Computes ROC AUC using self as actual and pred as predictions.</p> <p>Self must be binary and castable to type UInt32. If self is not all 0s and 1s or not binary, the result will not make sense, or some error may occur.</p>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.roc_auc--parameters","title":"Parameters","text":"<p>pred     An expression represeting the column with predicted probability.</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def roc_auc(self, pred: pl.Expr) -&gt; pl.Expr:\n    \"\"\"\n    Computes ROC AUC using self as actual and pred as predictions.\n\n    Self must be binary and castable to type UInt32. If self is not all 0s and 1s or not binary,\n    the result will not make sense, or some error may occur.\n\n    Parameters\n    ----------\n    pred\n        An expression represeting the column with predicted probability.\n    \"\"\"\n    y = self._expr.cast(pl.UInt32)\n    return y.register_plugin(\n        lib=_lib,\n        symbol=\"pl_roc_auc\",\n        args=[pred],\n        is_elementwise=False,\n        returns_scalar=True,\n    )\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.smape","title":"<code>smape(pred)</code>","text":"<p>Computes symmetric mean absolute percentage error between self and other <code>pred</code> expression. The value is always between 0 and 1. This is the third version in the wikipedia without the 100 factor.</p> <p>https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error</p>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.smape--parameters","title":"Parameters","text":"<p>pred     A Polars expression representing predictions</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def smape(self, pred: pl.Expr) -&gt; pl.Expr:\n    \"\"\"\n    Computes symmetric mean absolute percentage error between self and other `pred` expression.\n    The value is always between 0 and 1. This is the third version in the wikipedia without\n    the 100 factor.\n\n    https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error\n\n    Parameters\n    ----------\n    pred\n        A Polars expression representing predictions\n    \"\"\"\n    numerator = (self._expr - pred).abs()\n    denominator = 1.0 / (self._expr.abs() + pred.abs())\n    return (1.0 / self._expr.count()) * numerator.dot(denominator)\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.std_err","title":"<code>std_err(ddof=1)</code>","text":"<p>Estimates the standard error for the mean of the expression.</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def std_err(self, ddof: int = 1) -&gt; pl.Expr:\n    \"\"\"\n    Estimates the standard error for the mean of the expression.\n    \"\"\"\n    return self._expr.std(ddof=ddof) / self._expr.count().sqrt()\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.std_over_range","title":"<code>std_over_range(ddof=1)</code>","text":"<p>Computes the standard deviation over the range.</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def std_over_range(self, ddof: int = 1) -&gt; pl.Expr:\n    \"\"\"\n    Computes the standard deviation over the range.\n    \"\"\"\n    return self._expr.std(ddof=ddof) / (self._expr.max() - self._expr.min())\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.trapz","title":"<code>trapz(x)</code>","text":"<p>Treats self as y axis, integrates along x using the trapezoidal rule. If x is not a single value, then x should be sorted.</p>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.trapz--parameters","title":"Parameters","text":"<p>x     If it is a single float, it must be positive and it will represent a uniform     distance between points. If it is an expression, it must be sorted, does not contain     null, and have the same length as self.</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def trapz(self, x: Union[float, pl.Expr]) -&gt; pl.Expr:\n    \"\"\"\n    Treats self as y axis, integrates along x using the trapezoidal rule. If x is not a single\n    value, then x should be sorted.\n\n    Parameters\n    ----------\n    x\n        If it is a single float, it must be positive and it will represent a uniform\n        distance between points. If it is an expression, it must be sorted, does not contain\n        null, and have the same length as self.\n    \"\"\"\n    y = self._expr.cast(pl.Float64)\n    if isinstance(x, float):\n        x_ = pl.lit(abs(x), pl.Float64)\n    else:\n        x_ = x.cast(pl.Float64)\n\n    return y.register_plugin(\n        lib=_lib,\n        symbol=\"pl_trapz\",\n        args=[x_],\n        is_elementwise=False,\n        returns_scalar=True,\n    )\n</code></pre>"},{"location":"num_ext/#polars_ds.num_ext.NumExt.z_normalize","title":"<code>z_normalize(ddof=1)</code>","text":"<p>z_normalize the given expression: remove the mean and scales by the std</p> Source code in <code>python/polars_ds/num_ext.py</code> <pre><code>def z_normalize(self, ddof: int = 1) -&gt; pl.Expr:\n    \"\"\"\n    z_normalize the given expression: remove the mean and scales by the std\n    \"\"\"\n    return (self._expr - self._expr.mean()) / self._expr.std(ddof=ddof)\n</code></pre>"},{"location":"stats_ext/","title":"Stats Extension","text":""},{"location":"stats_ext/#extension-for-statistical-tests-and-samples","title":"Extension for Statistical Tests and Samples","text":""},{"location":"stats_ext/#polars_ds.stats_ext.StatsExt","title":"<code>StatsExt</code>","text":"<p>This class contains tools for dealing with well-known statistical tests and random sampling inside Polars DataFrame.</p> <p>Polars Namespace: stats_ext</p> <p>Example: pl.col(\"a\").stats_ext.ttest_ind(pl.col(\"b\"), equal_var = True)</p> Source code in <code>python/polars_ds/stats_ext.py</code> <pre><code>@pl.api.register_expr_namespace(\"stats_ext\")\nclass StatsExt:\n\n    \"\"\"\n    This class contains tools for dealing with well-known statistical tests and random sampling inside Polars DataFrame.\n\n    Polars Namespace: stats_ext\n\n    Example: pl.col(\"a\").stats_ext.ttest_ind(pl.col(\"b\"), equal_var = True)\n    \"\"\"\n\n    def __init__(self, expr: pl.Expr):\n        self._expr: pl.Expr = expr\n\n    def ttest_ind(\n        self, other: pl.Expr, alternative: Alternative = \"two-sided\", equal_var: bool = False\n    ) -&gt; pl.Expr:\n        \"\"\"\n        Performs 2 sample student's t test or Welch's t test. Functionality-wise this is\n        equivalent to SciPy's ttest_ind, with fewer options. The result is not exact but\n        within 1e-10 precision from SciPy's result.\n\n        In the case of student's t test, the data is assumed to have no nulls, and n = self._expr.count()\n        is used to compute the statistic. Note self._expr.count() only counts non-null elements after polars 0.20.\n        The df will be 2n - 2. As a result, nulls might cause problems.\n\n        In the case of Welch's t test, data will be sanitized (nulls, NaNs, Infs will be dropped\n        before the test), and df will be counted based on the length of sanitized data.\n\n        Parameters\n        ----------\n        other\n            The other expression\n        alternative\n            One of \"two-sided\", \"less\" or \"greater\"\n        equal_var\n            If true, perform standard student t 2 sample test. Otherwise, perform Welch's\n            t test.\n        \"\"\"\n        if equal_var:\n            m1 = self._expr.mean()\n            m2 = other.mean()\n            v1 = self._expr.var()\n            v2 = other.var()\n            cnt = self._expr.count().cast(pl.UInt64)\n            return m1.register_plugin(\n                lib=_lib,\n                symbol=\"pl_ttest_2samp\",\n                args=[m2, v1, v2, cnt, pl.lit(alternative, dtype=pl.Utf8)],\n                is_elementwise=False,\n                returns_scalar=True,\n            )\n        else:\n            s1 = self._expr.filter(self._expr.is_finite())\n            s2 = other.filter(other.is_finite())\n            m1 = s1.mean()\n            m2 = s2.mean()\n            v1 = s1.var()\n            v2 = s2.var()\n            n1 = s1.count().cast(pl.UInt64)\n            n2 = s2.count().cast(pl.UInt64)\n            return m1.register_plugin(\n                lib=_lib,\n                symbol=\"pl_welch_t\",\n                args=[m2, v1, v2, n1, n2, pl.lit(alternative, dtype=pl.Utf8)],\n                is_elementwise=False,\n                returns_scalar=True,\n            )\n\n    def ttest_1samp(self, pop_mean: float, alternative: Alternative = \"two-sided\") -&gt; pl.Expr:\n        \"\"\"\n        Performs a standard 1 sample t test using reference column and expected mean. This function\n        sanitizes the self column first. The df is the count of valid values.\n\n        Parameters\n        ----------\n        pop_mean\n            The expected population mean in the hypothesis test\n        alternative\n            One of \"two-sided\", \"less\" or \"greater\"\n        \"\"\"\n        s1 = self._expr.filter(self._expr.is_finite())\n        sm = s1.mean()\n        pm = pl.lit(pop_mean, dtype=pl.Float64)\n        var = s1.var()\n        cnt = s1.count().cast(pl.UInt64)\n        alt = pl.lit(alternative, dtype=pl.Utf8)\n        return sm.register_plugin(\n            lib=_lib,\n            symbol=\"pl_ttest_1samp\",\n            args=[pm, var, cnt, alt],\n            is_elementwise=False,\n            returns_scalar=True,\n        )\n\n    def f_stats(self, *cols: pl.Expr) -&gt; pl.Expr:\n        \"\"\"\n        Computes multiple F statistics at once using self as the grouping column. This does not\n        output p values. If the p value is desired, use f_test. This will return\n        all the stats as a scalar list in order.\n\n        Parameters\n        ----------\n        *cols\n            Polars expressions for numerical columns. The columns must be of the same length.\n        \"\"\"\n        return self._expr.register_plugin(\n            lib=_lib,\n            symbol=\"pl_f_stats\",\n            args=list(cols),\n            is_elementwise=False,\n            returns_scalar=True,\n        )\n\n    def f_test(self, other: pl.Expr) -&gt; pl.Expr:\n        \"\"\"\n        Performs the ANOVA F-test using self as the grouping column.\n\n        Parameters\n        ----------\n        other\n            The column to run ANOVA F-test on\n        \"\"\"\n        return self._expr.register_plugin(\n            lib=_lib,\n            symbol=\"pl_f_test\",\n            args=[other],\n            is_elementwise=False,\n            returns_scalar=True,\n        )\n\n    def normal_test(self) -&gt; pl.Expr:\n        \"\"\"\n        Perform a normality test which is based on D'Agostino and Pearson's test\n        that combines skew and kurtosis to produce an omnibus test of normality.\n        Null values, NaN and inf are dropped when running this computation.\n\n        References\n        ----------\n        D'Agostino, R. B. (1971), \"An omnibus test of normality for\n            moderate and large sample size\", Biometrika, 58, 341-348\n        D'Agostino, R. and Pearson, E. S. (1973), \"Tests for departure from\n            normality\", Biometrika, 60, 613-622\n        \"\"\"\n        valid: pl.Expr = self._expr.filter(self._expr.is_finite())\n        skew = valid.skew()\n        # Pearson Kurtosis, see here: https://en.wikipedia.org/wiki/D%27Agostino%27s_K-squared_test\n        kur = valid.kurtosis(fisher=False)\n        return skew.register_plugin(\n            lib=_lib,\n            symbol=\"pl_normal_test\",\n            args=[kur, valid.count().cast(pl.UInt64)],\n            is_elementwise=False,\n            returns_scalar=True,\n        )\n\n    def ks_stats(self, other: pl.Expr) -&gt; pl.Expr:\n        \"\"\"\n        Computes two-sided KS statistics with other. Currently it only returns the statistics.\n\n        Parameters\n        ----------\n        other\n            A Polars Expression\n        \"\"\"\n        y = self._expr.cast(pl.Float64)\n        other_ = other.cast(pl.Float64)\n        return y.register_plugin(\n            lib=_lib,\n            symbol=\"pl_ks_2samp\",\n            args=[other_, pl.lit(True, dtype=pl.Boolean)],\n            is_elementwise=False,\n            returns_scalar=True,\n        )\n\n    def ks_binary_classif(self, target: pl.Expr) -&gt; pl.Expr:\n        \"\"\"\n        Given a binary target, compute the ks statistics by comparing the feature where target = 1\n        with the same feature where target != 1.\n\n        Parameters\n        ----------\n        other\n            A Polars Expression\n        \"\"\"\n        y = self._expr.cast(pl.Float64)\n        y1 = y.filter(target == target.max())\n        y2 = y.filter((target == target.max()).not_())\n        return y1.register_plugin(\n            lib=_lib,\n            symbol=\"pl_ks_2samp\",\n            args=[y2, pl.lit(True, dtype=pl.Boolean)],\n            is_elementwise=False,\n            returns_scalar=True,\n        )\n\n    def rand_int(\n        self,\n        low: Optional[int] = 0,\n        high: Optional[int] = 10,\n        respect_null: bool = False,\n        use_ref: bool = False,\n    ) -&gt; pl.Expr:\n        \"\"\"\n        Generates random integers uniformly from the range [low, high). Throws an error if low == high\n        or if low is None and high is None and use_ref_nunique == False.\n\n        This treats self as the reference column.\n\n        Parameters\n        ----------\n        low\n            Lower end of random sample. None will be replaced 0.\n        high\n            Higher end of random sample. None will be replaced n_unique of reference.\n        respect_null\n            If true, null in reference column will be null in the new column\n        \"\"\"\n        if (low is None) &amp; (high is None):\n            raise ValueError(\"Either low or high must be set.\")\n\n        lo = pl.lit(low, dtype=pl.Int32)\n        hi = self._expr.n_unique.cast(pl.UInt32) if high is None else pl.lit(high, dtype=pl.Int32)\n        resp = pl.lit(respect_null, dtype=pl.Boolean)\n        return self._expr.register_plugin(\n            lib=_lib,\n            symbol=\"pl_rand_int\",\n            args=[lo, hi, resp],\n            is_elementwise=True,\n            returns_scalar=False,\n        )\n\n    def sample_uniform(\n        self, low: Optional[float] = None, high: Optional[float] = None, respect_null: bool = False\n    ) -&gt; pl.Expr:\n        \"\"\"\n        Creates self.len() many random points sampled from a uniform distribution within [low, high).\n        This will throw an error if low == high.\n\n        This treats self as the reference column.\n\n        Parameters\n        ----------\n        low\n            Lower end of random sample. If none, use reference col's min.\n        high\n            Higher end of random sample. If none, use reference col's max.\n        respect_null\n            If true, null in reference column will be null in the new column\n        \"\"\"\n\n        lo = self._expr.min() if low is None else pl.lit(low, dtype=pl.Float64)\n        hi = self._expr.max() if high is None else pl.lit(high, dtype=pl.Float64)\n        resp = pl.lit(respect_null, dtype=pl.Boolean)\n        return self._expr.register_plugin(\n            lib=_lib,\n            symbol=\"pl_sample_uniform\",\n            args=[lo, hi, resp],\n            is_elementwise=True,\n            returns_scalar=False,\n        )\n\n    def sample_binomial(self, n: int, p: float, respect_null: bool = False) -&gt; pl.Expr:\n        \"\"\"\n        Creates self.len() many random points sampled from a uniform binomial with n and p.\n\n        This treats self as the reference column.\n\n        Parameters\n        ----------\n        n\n            n in a binomial distribution\n        p\n            p in a binomial distribution\n        respect_null\n            If true, null in reference column will be null in the new column\n        \"\"\"\n\n        nn = pl.lit(n, dtype=pl.UInt64)\n        pp = pl.lit(p, dtype=pl.Float64)\n        resp = pl.lit(respect_null, dtype=pl.Boolean)\n        return self._expr.register_plugin(\n            lib=_lib,\n            symbol=\"pl_sample_binomial\",\n            args=[nn, pp, resp],\n            is_elementwise=True,\n            returns_scalar=False,\n        )\n\n    def sample_exp(self, lam: Optional[float] = None, respect_null: bool = False) -&gt; pl.Expr:\n        \"\"\"\n        Creates self.len() many random points sampled from a exponential distribution with n and p.\n\n        This treats self as the reference column.\n\n        Parameters\n        ----------\n        lam\n            lambda in a exponential distribution. If none, it will be 1/reference col's mean. Note that if\n            lambda &lt; 0 will throw an error and lambda = 0 will only return infinity.\n        respect_null\n            If true, null in reference column will be null in the new column\n        \"\"\"\n\n        lamb = (1.0 / self._expr.mean()) if lam is None else pl.lit(lam, dtype=pl.Float64)\n        resp = pl.lit(respect_null, dtype=pl.Boolean)\n        return self._expr.register_plugin(\n            lib=_lib,\n            symbol=\"pl_sample_exp\",\n            args=[lamb, resp],\n            is_elementwise=True,\n            returns_scalar=False,\n        )\n\n    def sample_normal(\n        self, mean: Optional[float] = None, std: Optional[float] = None, respect_null: bool = False\n    ) -&gt; pl.Expr:\n        \"\"\"\n        Creates self.len() many random points sampled from a normal distribution with the given\n        mean and std.\n\n        This treats self as the reference column.\n\n        Parameters\n        ----------\n        mean\n            Mean of the normal distribution. If none, use reference col's mean.\n        std\n            Std of the normal distribution. If none, use reference col's std.\n        respect_null\n            If true, null in reference column will be null in the new column\n        \"\"\"\n\n        me = self._expr.mean() if mean is None else pl.lit(mean, dtype=pl.Float64)\n        st = self._expr.std() if std is None else pl.lit(std, dtype=pl.Float64)\n        resp = pl.lit(respect_null, dtype=pl.Boolean)\n        return self._expr.register_plugin(\n            lib=_lib,\n            symbol=\"pl_sample_normal\",\n            args=[me, st, resp],\n            is_elementwise=True,\n            returns_scalar=False,\n        )\n\n    def rand_str(\n        self, min_size: int = 1, max_size: int = 10, respect_null: bool = False\n    ) -&gt; pl.Expr:\n        \"\"\"\n        Creates self.len() many random strings with alpha-numerical values. Unfortunately that\n        means this currently only generates strings satisfying [0-9a-zA-Z]. The string's\n        length will also be uniformly.\n\n        This treats self as the reference column.\n\n        Parameters\n        ----------\n        min_size\n            The minimum length of the string to be generated. The length of the string will be\n            uniformly generated in [min_size, max_size), except when min_size = max_size, in\n            which case only fixed length strings will be generated.\n        max_size\n            The maximum length of the string to be generated.\n        respect_null\n            If true, null in reference column will be null in the new column\n        \"\"\"\n        if max_size &lt;= 0:\n            raise ValueError(\"Input `max_size` must be positive.\")\n\n        min_s = pl.lit(min_size, dtype=pl.UInt32)\n        max_s = pl.lit(max_size, dtype=pl.UInt32)\n        resp = pl.lit(respect_null, dtype=pl.Boolean)\n        return self._expr.register_plugin(\n            lib=_lib,\n            symbol=\"pl_sample_alphanumeric\",\n            args=[min_s, max_s, resp],\n            is_elementwise=True,\n            returns_scalar=False,\n        )\n</code></pre>"},{"location":"stats_ext/#polars_ds.stats_ext.StatsExt.f_stats","title":"<code>f_stats(*cols)</code>","text":"<p>Computes multiple F statistics at once using self as the grouping column. This does not output p values. If the p value is desired, use f_test. This will return all the stats as a scalar list in order.</p>"},{"location":"stats_ext/#polars_ds.stats_ext.StatsExt.f_stats--parameters","title":"Parameters","text":"<p>*cols     Polars expressions for numerical columns. The columns must be of the same length.</p> Source code in <code>python/polars_ds/stats_ext.py</code> <pre><code>def f_stats(self, *cols: pl.Expr) -&gt; pl.Expr:\n    \"\"\"\n    Computes multiple F statistics at once using self as the grouping column. This does not\n    output p values. If the p value is desired, use f_test. This will return\n    all the stats as a scalar list in order.\n\n    Parameters\n    ----------\n    *cols\n        Polars expressions for numerical columns. The columns must be of the same length.\n    \"\"\"\n    return self._expr.register_plugin(\n        lib=_lib,\n        symbol=\"pl_f_stats\",\n        args=list(cols),\n        is_elementwise=False,\n        returns_scalar=True,\n    )\n</code></pre>"},{"location":"stats_ext/#polars_ds.stats_ext.StatsExt.f_test","title":"<code>f_test(other)</code>","text":"<p>Performs the ANOVA F-test using self as the grouping column.</p>"},{"location":"stats_ext/#polars_ds.stats_ext.StatsExt.f_test--parameters","title":"Parameters","text":"<p>other     The column to run ANOVA F-test on</p> Source code in <code>python/polars_ds/stats_ext.py</code> <pre><code>def f_test(self, other: pl.Expr) -&gt; pl.Expr:\n    \"\"\"\n    Performs the ANOVA F-test using self as the grouping column.\n\n    Parameters\n    ----------\n    other\n        The column to run ANOVA F-test on\n    \"\"\"\n    return self._expr.register_plugin(\n        lib=_lib,\n        symbol=\"pl_f_test\",\n        args=[other],\n        is_elementwise=False,\n        returns_scalar=True,\n    )\n</code></pre>"},{"location":"stats_ext/#polars_ds.stats_ext.StatsExt.ks_binary_classif","title":"<code>ks_binary_classif(target)</code>","text":"<p>Given a binary target, compute the ks statistics by comparing the feature where target = 1 with the same feature where target != 1.</p>"},{"location":"stats_ext/#polars_ds.stats_ext.StatsExt.ks_binary_classif--parameters","title":"Parameters","text":"<p>other     A Polars Expression</p> Source code in <code>python/polars_ds/stats_ext.py</code> <pre><code>def ks_binary_classif(self, target: pl.Expr) -&gt; pl.Expr:\n    \"\"\"\n    Given a binary target, compute the ks statistics by comparing the feature where target = 1\n    with the same feature where target != 1.\n\n    Parameters\n    ----------\n    other\n        A Polars Expression\n    \"\"\"\n    y = self._expr.cast(pl.Float64)\n    y1 = y.filter(target == target.max())\n    y2 = y.filter((target == target.max()).not_())\n    return y1.register_plugin(\n        lib=_lib,\n        symbol=\"pl_ks_2samp\",\n        args=[y2, pl.lit(True, dtype=pl.Boolean)],\n        is_elementwise=False,\n        returns_scalar=True,\n    )\n</code></pre>"},{"location":"stats_ext/#polars_ds.stats_ext.StatsExt.ks_stats","title":"<code>ks_stats(other)</code>","text":"<p>Computes two-sided KS statistics with other. Currently it only returns the statistics.</p>"},{"location":"stats_ext/#polars_ds.stats_ext.StatsExt.ks_stats--parameters","title":"Parameters","text":"<p>other     A Polars Expression</p> Source code in <code>python/polars_ds/stats_ext.py</code> <pre><code>def ks_stats(self, other: pl.Expr) -&gt; pl.Expr:\n    \"\"\"\n    Computes two-sided KS statistics with other. Currently it only returns the statistics.\n\n    Parameters\n    ----------\n    other\n        A Polars Expression\n    \"\"\"\n    y = self._expr.cast(pl.Float64)\n    other_ = other.cast(pl.Float64)\n    return y.register_plugin(\n        lib=_lib,\n        symbol=\"pl_ks_2samp\",\n        args=[other_, pl.lit(True, dtype=pl.Boolean)],\n        is_elementwise=False,\n        returns_scalar=True,\n    )\n</code></pre>"},{"location":"stats_ext/#polars_ds.stats_ext.StatsExt.normal_test","title":"<code>normal_test()</code>","text":"<p>Perform a normality test which is based on D'Agostino and Pearson's test that combines skew and kurtosis to produce an omnibus test of normality. Null values, NaN and inf are dropped when running this computation.</p>"},{"location":"stats_ext/#polars_ds.stats_ext.StatsExt.normal_test--references","title":"References","text":"<p>D'Agostino, R. B. (1971), \"An omnibus test of normality for     moderate and large sample size\", Biometrika, 58, 341-348 D'Agostino, R. and Pearson, E. S. (1973), \"Tests for departure from     normality\", Biometrika, 60, 613-622</p> Source code in <code>python/polars_ds/stats_ext.py</code> <pre><code>def normal_test(self) -&gt; pl.Expr:\n    \"\"\"\n    Perform a normality test which is based on D'Agostino and Pearson's test\n    that combines skew and kurtosis to produce an omnibus test of normality.\n    Null values, NaN and inf are dropped when running this computation.\n\n    References\n    ----------\n    D'Agostino, R. B. (1971), \"An omnibus test of normality for\n        moderate and large sample size\", Biometrika, 58, 341-348\n    D'Agostino, R. and Pearson, E. S. (1973), \"Tests for departure from\n        normality\", Biometrika, 60, 613-622\n    \"\"\"\n    valid: pl.Expr = self._expr.filter(self._expr.is_finite())\n    skew = valid.skew()\n    # Pearson Kurtosis, see here: https://en.wikipedia.org/wiki/D%27Agostino%27s_K-squared_test\n    kur = valid.kurtosis(fisher=False)\n    return skew.register_plugin(\n        lib=_lib,\n        symbol=\"pl_normal_test\",\n        args=[kur, valid.count().cast(pl.UInt64)],\n        is_elementwise=False,\n        returns_scalar=True,\n    )\n</code></pre>"},{"location":"stats_ext/#polars_ds.stats_ext.StatsExt.rand_int","title":"<code>rand_int(low=0, high=10, respect_null=False, use_ref=False)</code>","text":"<p>Generates random integers uniformly from the range [low, high). Throws an error if low == high or if low is None and high is None and use_ref_nunique == False.</p> <p>This treats self as the reference column.</p>"},{"location":"stats_ext/#polars_ds.stats_ext.StatsExt.rand_int--parameters","title":"Parameters","text":"<p>low     Lower end of random sample. None will be replaced 0. high     Higher end of random sample. None will be replaced n_unique of reference. respect_null     If true, null in reference column will be null in the new column</p> Source code in <code>python/polars_ds/stats_ext.py</code> <pre><code>def rand_int(\n    self,\n    low: Optional[int] = 0,\n    high: Optional[int] = 10,\n    respect_null: bool = False,\n    use_ref: bool = False,\n) -&gt; pl.Expr:\n    \"\"\"\n    Generates random integers uniformly from the range [low, high). Throws an error if low == high\n    or if low is None and high is None and use_ref_nunique == False.\n\n    This treats self as the reference column.\n\n    Parameters\n    ----------\n    low\n        Lower end of random sample. None will be replaced 0.\n    high\n        Higher end of random sample. None will be replaced n_unique of reference.\n    respect_null\n        If true, null in reference column will be null in the new column\n    \"\"\"\n    if (low is None) &amp; (high is None):\n        raise ValueError(\"Either low or high must be set.\")\n\n    lo = pl.lit(low, dtype=pl.Int32)\n    hi = self._expr.n_unique.cast(pl.UInt32) if high is None else pl.lit(high, dtype=pl.Int32)\n    resp = pl.lit(respect_null, dtype=pl.Boolean)\n    return self._expr.register_plugin(\n        lib=_lib,\n        symbol=\"pl_rand_int\",\n        args=[lo, hi, resp],\n        is_elementwise=True,\n        returns_scalar=False,\n    )\n</code></pre>"},{"location":"stats_ext/#polars_ds.stats_ext.StatsExt.rand_str","title":"<code>rand_str(min_size=1, max_size=10, respect_null=False)</code>","text":"<p>Creates self.len() many random strings with alpha-numerical values. Unfortunately that means this currently only generates strings satisfying [0-9a-zA-Z]. The string's length will also be uniformly.</p> <p>This treats self as the reference column.</p>"},{"location":"stats_ext/#polars_ds.stats_ext.StatsExt.rand_str--parameters","title":"Parameters","text":"<p>min_size     The minimum length of the string to be generated. The length of the string will be     uniformly generated in [min_size, max_size), except when min_size = max_size, in     which case only fixed length strings will be generated. max_size     The maximum length of the string to be generated. respect_null     If true, null in reference column will be null in the new column</p> Source code in <code>python/polars_ds/stats_ext.py</code> <pre><code>def rand_str(\n    self, min_size: int = 1, max_size: int = 10, respect_null: bool = False\n) -&gt; pl.Expr:\n    \"\"\"\n    Creates self.len() many random strings with alpha-numerical values. Unfortunately that\n    means this currently only generates strings satisfying [0-9a-zA-Z]. The string's\n    length will also be uniformly.\n\n    This treats self as the reference column.\n\n    Parameters\n    ----------\n    min_size\n        The minimum length of the string to be generated. The length of the string will be\n        uniformly generated in [min_size, max_size), except when min_size = max_size, in\n        which case only fixed length strings will be generated.\n    max_size\n        The maximum length of the string to be generated.\n    respect_null\n        If true, null in reference column will be null in the new column\n    \"\"\"\n    if max_size &lt;= 0:\n        raise ValueError(\"Input `max_size` must be positive.\")\n\n    min_s = pl.lit(min_size, dtype=pl.UInt32)\n    max_s = pl.lit(max_size, dtype=pl.UInt32)\n    resp = pl.lit(respect_null, dtype=pl.Boolean)\n    return self._expr.register_plugin(\n        lib=_lib,\n        symbol=\"pl_sample_alphanumeric\",\n        args=[min_s, max_s, resp],\n        is_elementwise=True,\n        returns_scalar=False,\n    )\n</code></pre>"},{"location":"stats_ext/#polars_ds.stats_ext.StatsExt.sample_binomial","title":"<code>sample_binomial(n, p, respect_null=False)</code>","text":"<p>Creates self.len() many random points sampled from a uniform binomial with n and p.</p> <p>This treats self as the reference column.</p>"},{"location":"stats_ext/#polars_ds.stats_ext.StatsExt.sample_binomial--parameters","title":"Parameters","text":"<p>n     n in a binomial distribution p     p in a binomial distribution respect_null     If true, null in reference column will be null in the new column</p> Source code in <code>python/polars_ds/stats_ext.py</code> <pre><code>def sample_binomial(self, n: int, p: float, respect_null: bool = False) -&gt; pl.Expr:\n    \"\"\"\n    Creates self.len() many random points sampled from a uniform binomial with n and p.\n\n    This treats self as the reference column.\n\n    Parameters\n    ----------\n    n\n        n in a binomial distribution\n    p\n        p in a binomial distribution\n    respect_null\n        If true, null in reference column will be null in the new column\n    \"\"\"\n\n    nn = pl.lit(n, dtype=pl.UInt64)\n    pp = pl.lit(p, dtype=pl.Float64)\n    resp = pl.lit(respect_null, dtype=pl.Boolean)\n    return self._expr.register_plugin(\n        lib=_lib,\n        symbol=\"pl_sample_binomial\",\n        args=[nn, pp, resp],\n        is_elementwise=True,\n        returns_scalar=False,\n    )\n</code></pre>"},{"location":"stats_ext/#polars_ds.stats_ext.StatsExt.sample_exp","title":"<code>sample_exp(lam=None, respect_null=False)</code>","text":"<p>Creates self.len() many random points sampled from a exponential distribution with n and p.</p> <p>This treats self as the reference column.</p>"},{"location":"stats_ext/#polars_ds.stats_ext.StatsExt.sample_exp--parameters","title":"Parameters","text":"<p>lam     lambda in a exponential distribution. If none, it will be 1/reference col's mean. Note that if     lambda &lt; 0 will throw an error and lambda = 0 will only return infinity. respect_null     If true, null in reference column will be null in the new column</p> Source code in <code>python/polars_ds/stats_ext.py</code> <pre><code>def sample_exp(self, lam: Optional[float] = None, respect_null: bool = False) -&gt; pl.Expr:\n    \"\"\"\n    Creates self.len() many random points sampled from a exponential distribution with n and p.\n\n    This treats self as the reference column.\n\n    Parameters\n    ----------\n    lam\n        lambda in a exponential distribution. If none, it will be 1/reference col's mean. Note that if\n        lambda &lt; 0 will throw an error and lambda = 0 will only return infinity.\n    respect_null\n        If true, null in reference column will be null in the new column\n    \"\"\"\n\n    lamb = (1.0 / self._expr.mean()) if lam is None else pl.lit(lam, dtype=pl.Float64)\n    resp = pl.lit(respect_null, dtype=pl.Boolean)\n    return self._expr.register_plugin(\n        lib=_lib,\n        symbol=\"pl_sample_exp\",\n        args=[lamb, resp],\n        is_elementwise=True,\n        returns_scalar=False,\n    )\n</code></pre>"},{"location":"stats_ext/#polars_ds.stats_ext.StatsExt.sample_normal","title":"<code>sample_normal(mean=None, std=None, respect_null=False)</code>","text":"<p>Creates self.len() many random points sampled from a normal distribution with the given mean and std.</p> <p>This treats self as the reference column.</p>"},{"location":"stats_ext/#polars_ds.stats_ext.StatsExt.sample_normal--parameters","title":"Parameters","text":"<p>mean     Mean of the normal distribution. If none, use reference col's mean. std     Std of the normal distribution. If none, use reference col's std. respect_null     If true, null in reference column will be null in the new column</p> Source code in <code>python/polars_ds/stats_ext.py</code> <pre><code>def sample_normal(\n    self, mean: Optional[float] = None, std: Optional[float] = None, respect_null: bool = False\n) -&gt; pl.Expr:\n    \"\"\"\n    Creates self.len() many random points sampled from a normal distribution with the given\n    mean and std.\n\n    This treats self as the reference column.\n\n    Parameters\n    ----------\n    mean\n        Mean of the normal distribution. If none, use reference col's mean.\n    std\n        Std of the normal distribution. If none, use reference col's std.\n    respect_null\n        If true, null in reference column will be null in the new column\n    \"\"\"\n\n    me = self._expr.mean() if mean is None else pl.lit(mean, dtype=pl.Float64)\n    st = self._expr.std() if std is None else pl.lit(std, dtype=pl.Float64)\n    resp = pl.lit(respect_null, dtype=pl.Boolean)\n    return self._expr.register_plugin(\n        lib=_lib,\n        symbol=\"pl_sample_normal\",\n        args=[me, st, resp],\n        is_elementwise=True,\n        returns_scalar=False,\n    )\n</code></pre>"},{"location":"stats_ext/#polars_ds.stats_ext.StatsExt.sample_uniform","title":"<code>sample_uniform(low=None, high=None, respect_null=False)</code>","text":"<p>Creates self.len() many random points sampled from a uniform distribution within [low, high). This will throw an error if low == high.</p> <p>This treats self as the reference column.</p>"},{"location":"stats_ext/#polars_ds.stats_ext.StatsExt.sample_uniform--parameters","title":"Parameters","text":"<p>low     Lower end of random sample. If none, use reference col's min. high     Higher end of random sample. If none, use reference col's max. respect_null     If true, null in reference column will be null in the new column</p> Source code in <code>python/polars_ds/stats_ext.py</code> <pre><code>def sample_uniform(\n    self, low: Optional[float] = None, high: Optional[float] = None, respect_null: bool = False\n) -&gt; pl.Expr:\n    \"\"\"\n    Creates self.len() many random points sampled from a uniform distribution within [low, high).\n    This will throw an error if low == high.\n\n    This treats self as the reference column.\n\n    Parameters\n    ----------\n    low\n        Lower end of random sample. If none, use reference col's min.\n    high\n        Higher end of random sample. If none, use reference col's max.\n    respect_null\n        If true, null in reference column will be null in the new column\n    \"\"\"\n\n    lo = self._expr.min() if low is None else pl.lit(low, dtype=pl.Float64)\n    hi = self._expr.max() if high is None else pl.lit(high, dtype=pl.Float64)\n    resp = pl.lit(respect_null, dtype=pl.Boolean)\n    return self._expr.register_plugin(\n        lib=_lib,\n        symbol=\"pl_sample_uniform\",\n        args=[lo, hi, resp],\n        is_elementwise=True,\n        returns_scalar=False,\n    )\n</code></pre>"},{"location":"stats_ext/#polars_ds.stats_ext.StatsExt.ttest_1samp","title":"<code>ttest_1samp(pop_mean, alternative='two-sided')</code>","text":"<p>Performs a standard 1 sample t test using reference column and expected mean. This function sanitizes the self column first. The df is the count of valid values.</p>"},{"location":"stats_ext/#polars_ds.stats_ext.StatsExt.ttest_1samp--parameters","title":"Parameters","text":"<p>pop_mean     The expected population mean in the hypothesis test alternative     One of \"two-sided\", \"less\" or \"greater\"</p> Source code in <code>python/polars_ds/stats_ext.py</code> <pre><code>def ttest_1samp(self, pop_mean: float, alternative: Alternative = \"two-sided\") -&gt; pl.Expr:\n    \"\"\"\n    Performs a standard 1 sample t test using reference column and expected mean. This function\n    sanitizes the self column first. The df is the count of valid values.\n\n    Parameters\n    ----------\n    pop_mean\n        The expected population mean in the hypothesis test\n    alternative\n        One of \"two-sided\", \"less\" or \"greater\"\n    \"\"\"\n    s1 = self._expr.filter(self._expr.is_finite())\n    sm = s1.mean()\n    pm = pl.lit(pop_mean, dtype=pl.Float64)\n    var = s1.var()\n    cnt = s1.count().cast(pl.UInt64)\n    alt = pl.lit(alternative, dtype=pl.Utf8)\n    return sm.register_plugin(\n        lib=_lib,\n        symbol=\"pl_ttest_1samp\",\n        args=[pm, var, cnt, alt],\n        is_elementwise=False,\n        returns_scalar=True,\n    )\n</code></pre>"},{"location":"stats_ext/#polars_ds.stats_ext.StatsExt.ttest_ind","title":"<code>ttest_ind(other, alternative='two-sided', equal_var=False)</code>","text":"<p>Performs 2 sample student's t test or Welch's t test. Functionality-wise this is equivalent to SciPy's ttest_ind, with fewer options. The result is not exact but within 1e-10 precision from SciPy's result.</p> <p>In the case of student's t test, the data is assumed to have no nulls, and n = self._expr.count() is used to compute the statistic. Note self._expr.count() only counts non-null elements after polars 0.20. The df will be 2n - 2. As a result, nulls might cause problems.</p> <p>In the case of Welch's t test, data will be sanitized (nulls, NaNs, Infs will be dropped before the test), and df will be counted based on the length of sanitized data.</p>"},{"location":"stats_ext/#polars_ds.stats_ext.StatsExt.ttest_ind--parameters","title":"Parameters","text":"<p>other     The other expression alternative     One of \"two-sided\", \"less\" or \"greater\" equal_var     If true, perform standard student t 2 sample test. Otherwise, perform Welch's     t test.</p> Source code in <code>python/polars_ds/stats_ext.py</code> <pre><code>def ttest_ind(\n    self, other: pl.Expr, alternative: Alternative = \"two-sided\", equal_var: bool = False\n) -&gt; pl.Expr:\n    \"\"\"\n    Performs 2 sample student's t test or Welch's t test. Functionality-wise this is\n    equivalent to SciPy's ttest_ind, with fewer options. The result is not exact but\n    within 1e-10 precision from SciPy's result.\n\n    In the case of student's t test, the data is assumed to have no nulls, and n = self._expr.count()\n    is used to compute the statistic. Note self._expr.count() only counts non-null elements after polars 0.20.\n    The df will be 2n - 2. As a result, nulls might cause problems.\n\n    In the case of Welch's t test, data will be sanitized (nulls, NaNs, Infs will be dropped\n    before the test), and df will be counted based on the length of sanitized data.\n\n    Parameters\n    ----------\n    other\n        The other expression\n    alternative\n        One of \"two-sided\", \"less\" or \"greater\"\n    equal_var\n        If true, perform standard student t 2 sample test. Otherwise, perform Welch's\n        t test.\n    \"\"\"\n    if equal_var:\n        m1 = self._expr.mean()\n        m2 = other.mean()\n        v1 = self._expr.var()\n        v2 = other.var()\n        cnt = self._expr.count().cast(pl.UInt64)\n        return m1.register_plugin(\n            lib=_lib,\n            symbol=\"pl_ttest_2samp\",\n            args=[m2, v1, v2, cnt, pl.lit(alternative, dtype=pl.Utf8)],\n            is_elementwise=False,\n            returns_scalar=True,\n        )\n    else:\n        s1 = self._expr.filter(self._expr.is_finite())\n        s2 = other.filter(other.is_finite())\n        m1 = s1.mean()\n        m2 = s2.mean()\n        v1 = s1.var()\n        v2 = s2.var()\n        n1 = s1.count().cast(pl.UInt64)\n        n2 = s2.count().cast(pl.UInt64)\n        return m1.register_plugin(\n            lib=_lib,\n            symbol=\"pl_welch_t\",\n            args=[m2, v1, v2, n1, n2, pl.lit(alternative, dtype=pl.Utf8)],\n            is_elementwise=False,\n            returns_scalar=True,\n        )\n</code></pre>"},{"location":"str_ext/","title":"String Extension","text":""},{"location":"str_ext/#extension-for-string-manipulation-and-metrics","title":"Extension for String Manipulation and Metrics","text":""},{"location":"str_ext/#polars_ds.str_ext.StrExt","title":"<code>StrExt</code>","text":"<p>This class contains tools for dealing with string similarity, common string operations like tokenize, extract numbers, etc., inside Polars DataFrame.</p> <p>Polars Namespace: str_ext</p> <p>Example: pl.col(\"a\").str_ext.levenshtein(pl.col(\"b\"), return_sim=True)</p> Source code in <code>python/polars_ds/str_ext.py</code> <pre><code>@pl.api.register_expr_namespace(\"str_ext\")\nclass StrExt:\n\n    \"\"\"\n    This class contains tools for dealing with string similarity, common string operations like tokenize,\n    extract numbers, etc., inside Polars DataFrame.\n\n    Polars Namespace: str_ext\n\n    Example: pl.col(\"a\").str_ext.levenshtein(pl.col(\"b\"), return_sim=True)\n    \"\"\"\n\n    def __init__(self, expr: pl.Expr):\n        self._expr: pl.Expr = expr\n\n    def is_stopword(self) -&gt; pl.Expr:\n        \"\"\"\n        Checks whether the string is a stopword or not.\n        \"\"\"\n        self._expr.register_plugin(\n            lib=_lib,\n            symbol=\"pl_is_stopword\",\n            args=[],\n            is_elementwise=True,\n        )\n\n    def extract_numbers(\n        self, ignore_comma: bool = False, join_by: str = \"\", dtype: pl.DataType = pl.Utf8\n    ) -&gt; pl.Expr:\n        \"\"\"\n        Extracts numbers from the string column, and stores them in a list.\n\n        Parameters\n        ----------\n        ignore_comma\n            Whether to remove all comma before matching for numbers\n        join_by\n            If dtype is pl.Utf8, join the list of strings using the value given here\n        dtype\n            The desired inner dtype for the extracted data. Should either be one of\n            pl.NUMERIC_DTYPES or pl.Utf8\n\n        Examples\n        --------\n        &gt;&gt;&gt; df = pl.DataFrame({\n        ...     \"survey\":[\"0% of my time\", \"1% to 25% of my time\", \"75% to 99% of my time\",\n        ...            \"50% to 74% of my time\", \"75% to 99% of my time\",\n        ...            \"50% to 74% of my time\"]\n        ... })\n        &gt;&gt;&gt; df.select(pl.col(\"survey\").str_ext.extract_numbers(dtype=pl.UInt32))\n        shape: (6, 1)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 survey    \u2502\n        \u2502 ---       \u2502\n        \u2502 list[u32] \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 [0]       \u2502\n        \u2502 [1, 25]   \u2502\n        \u2502 [75, 99]  \u2502\n        \u2502 [50, 74]  \u2502\n        \u2502 [75, 99]  \u2502\n        \u2502 [50, 74]  \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        &gt;&gt;&gt; df.select(pl.col(\"survey\").str_ext.extract_numbers(join_by=\"-\", dtype=pl.Utf8))\n        shape: (6, 1)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 survey \u2502\n        \u2502 ---    \u2502\n        \u2502 str    \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 0      \u2502\n        \u2502 1-25   \u2502\n        \u2502 75-99  \u2502\n        \u2502 50-74  \u2502\n        \u2502 75-99  \u2502\n        \u2502 50-74  \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \"\"\"\n        expr = self._expr\n        if ignore_comma:\n            expr = expr.str.replace_all(\",\", \"\")\n\n        # Find all numbers\n        expr = expr.str.extract_all(\"(\\d*\\.?\\d+)\")\n        if dtype in pl.NUMERIC_DTYPES:\n            expr = expr.list.eval(pl.element().cast(dtype))\n        elif dtype == pl.Utf8:  # As a list of strings\n            if join_by != \"\":\n                expr = expr.list.join(join_by)\n\n        return expr\n\n    def line_count(self) -&gt; pl.Expr:\n        \"\"\"\n        Return the line count of the string column.\n        \"\"\"\n        return self._expr.str.count_matches(pattern=\"\\n\")\n\n    def infer_infreq(\n        self,\n        *,\n        min_count: Optional[int] = None,\n        min_frac: Optional[float] = None,\n        parallel: bool = False,\n    ) -&gt; pl.Expr:\n        \"\"\"\n        Infers infrequent categories (strings) by min_count or min_frac and return a list as output.\n\n        Parameters\n        ----------\n        min_count\n            If set, an infrequency category will be defined as a category with count &lt; this.\n        min_frac\n            If set, an infrequency category will be defined as a category with pct &lt; this. min_count\n            takes priority over this.\n        parallel\n            Whether to run value_counts in parallel. This may not provide much speed up and is not\n            recommended in a group_by context.\n        \"\"\"\n        name = self._expr.meta.root_names()[0]\n        vc = self._expr.value_counts(parallel=parallel, sort=True)\n        if min_count is None and min_frac is None:\n            raise ValueError(\"Either min_count or min_frac must be provided.\")\n        elif min_count is not None:\n            infreq: pl.Expr = vc.filter(vc.struct.field(\"count\") &lt; min_count).struct.field(name)\n        elif min_frac is not None:\n            infreq: pl.Expr = vc.filter(\n                vc.struct.field(\"count\") / vc.struct.field(\"count\").sum() &lt; min_frac\n            ).struct.field(name)\n\n        return infreq.implode()\n\n    def merge_infreq(\n        self,\n        *,\n        min_count: Optional[int] = None,\n        min_frac: Optional[float] = None,\n        separator: str = \"|\",\n        parallel: bool = False,\n    ) -&gt; pl.Expr:\n        \"\"\"\n        Merge infrequent categories (strings) in the column into one category (string) separated by a\n        separator. This is useful when you want to do one-hot-encoding but do not want too many distinct\n        values because of low count values. However, this does not mean that the categories are similar\n        with respect to the your modelling problem.\n\n        Parameters\n        ----------\n        min_count\n            If set, an infrequency category will be defined as a category with count &lt; this.\n        min_frac\n            If set, an infrequency category will be defined as a category with pct &lt; this. min_count\n            takes priority over this.\n        separator\n            What separator to use when joining the categories. E.g if \"a\" and \"b\" are rare categories,\n            and separator = \"|\", they will be mapped to \"a|b\"\n        parallel\n            Whether to run value_counts in parallel. This may not provide much speed up and is not\n            recommended in a group_by context.\n        \"\"\"\n\n        # Will be fixed soon and sort will not be needed\n        name = self._expr.meta.root_names()[0]\n        vc = self._expr.value_counts(parallel=parallel, sort=True)\n        if min_count is None and min_frac is None:\n            raise ValueError(\"Either min_count or min_frac must be provided.\")\n        elif min_count is not None:\n            to_merge: pl.Expr = vc.filter(vc.struct.field(\"count\") &lt; min_count).struct.field(name)\n        elif min_frac is not None:\n            to_merge: pl.Expr = vc.filter(\n                vc.struct.field(\"count\") / vc.struct.field(\"count\").sum() &lt; min_frac\n            ).struct.field(name)\n\n        return (\n            pl.when(self._expr.is_in(to_merge))\n            .then(to_merge.cast(pl.Utf8).fill_null(\"null\").implode().first().list.join(separator))\n            .otherwise(self._expr)\n        )\n\n    def str_jaccard(\n        self, other: Union[str, pl.Expr], substr_size: int = 2, parallel: bool = False\n    ) -&gt; pl.Expr:\n        \"\"\"\n        Treats substrings of size `substr_size` as a set. And computes the jaccard similarity between\n        this word and the other. This is not the same as comparing bigrams.\n\n        Parameters\n        ----------\n        other\n            If this is a string, then the entire column will be compared with this string. If this\n            is an expression, then perform element-wise jaccard similarity computation between this column\n            and the other (given by the expression).\n        substr_size\n            The substring size for Jaccard similarity. E.g. if substr_size = 2, \"apple\" will be decomposed into\n            the set ('ap', 'pp', 'pl', 'le') before being compared.\n        parallel\n            Whether to run the comparisons in parallel. Note that this is not always faster, especially\n            when used with other expressions or in group_by/over context.\n        \"\"\"\n        if isinstance(other, str):\n            other_ = pl.lit(other, dtype=pl.Utf8)\n        else:\n            other_ = other\n\n        return self._expr.register_plugin(\n            lib=_lib,\n            symbol=\"pl_str_jaccard\",\n            args=[other_, pl.lit(substr_size, pl.UInt32), pl.lit(parallel, pl.Boolean)],\n            is_elementwise=True,\n        )\n\n    def sorensen_dice(\n        self, other: Union[str, pl.Expr], substr_size: int = 2, parallel: bool = False\n    ) -&gt; pl.Expr:\n        \"\"\"\n        Treats substrings of size `substr_size` as a set. And computes the Sorensen-Dice similarity between\n        this word and the other. This is not the same as comparing bigrams.\n\n        Parameters\n        ----------\n        other\n            If this is a string, then the entire column will be compared with this string. If this\n            is an expression, then perform element-wise jaccard similarity computation between this column\n            and the other (given by the expression).\n        substr_size\n            The substring size for Jaccard similarity. E.g. if substr_size = 2, \"apple\" will be decomposed into\n            the set ('ap', 'pp', 'pl', 'le') before being compared.\n        parallel\n            Whether to run the comparisons in parallel. Note that this is not always faster, especially\n            when used with other expressions or in group_by/over context.\n        \"\"\"\n        if isinstance(other, str):\n            other_ = pl.lit(other, dtype=pl.Utf8)\n        else:\n            other_ = other\n\n        return self._expr.register_plugin(\n            lib=_lib,\n            symbol=\"pl_sorensen_dice\",\n            args=[other_, pl.lit(substr_size, pl.UInt32), pl.lit(parallel, pl.Boolean)],\n            is_elementwise=True,\n        )\n\n    def overlap_coeff(\n        self, other: Union[str, pl.Expr], substr_size: int = 2, parallel: bool = False\n    ) -&gt; pl.Expr:\n        \"\"\"\n        Treats substrings of size `substr_size` as a set. And computes the overlap coefficient as\n        similarity between this word and the other. This is not the same as comparing bigrams.\n\n        Parameters\n        ----------\n        other\n            If this is a string, then the entire column will be compared with this string. If this\n            is an expression, then perform element-wise jaccard similarity computation between this column\n            and the other (given by the expression).\n        substr_size\n            The substring size for Jaccard similarity. E.g. if substr_size = 2, \"apple\" will be decomposed into\n            the set ('ap', 'pp', 'pl', 'le') before being compared.\n        parallel\n            Whether to run the comparisons in parallel. Note that this is not always faster, especially\n            when used with other expressions or in group_by/over context.\n        \"\"\"\n        if isinstance(other, str):\n            other_ = pl.lit(other, pl.Utf8)\n        else:\n            other_ = other\n\n        return self._expr.register_plugin(\n            lib=_lib,\n            symbol=\"pl_overlap_coeff\",\n            args=[other_, pl.lit(substr_size, pl.UInt32), pl.lit(parallel, pl.Boolean)],\n            is_elementwise=True,\n        )\n\n    def levenshtein(\n        self, other: Union[str, pl.Expr], parallel: bool = False, return_sim: bool = False\n    ) -&gt; pl.Expr:\n        \"\"\"\n        Computes the Levenshtein distance between this and the other str.\n\n        Parameters\n        ----------\n        other\n            If this is a string, then the entire column will be compared with this string. If this\n            is an expression, then an element-wise Levenshtein distance computation between this column\n            and the other (given by the expression) will be performed.\n        parallel\n            Whether to run the comparisons in parallel. Note that this is not always faster, especially\n            when used with other expressions or in group_by/over context.\n        return_sim\n            If true, return normalized Levenshtein.\n        \"\"\"\n        if isinstance(other, str):\n            other_ = pl.lit(other, dtype=pl.Utf8)\n        else:\n            other_ = other\n\n        if return_sim:\n            return self._expr.register_plugin(\n                lib=_lib,\n                symbol=\"pl_levenshtein_sim\",\n                args=[other_, pl.lit(parallel, pl.Boolean)],\n                is_elementwise=True,\n            )\n        else:\n            return self._expr.register_plugin(\n                lib=_lib,\n                symbol=\"pl_levenshtein\",\n                args=[other_, pl.lit(parallel, pl.Boolean)],\n                is_elementwise=True,\n            )\n\n    def levenshtein_within(\n        self,\n        other: Union[str, pl.Expr],\n        bound: int,\n        parallel: bool = False,\n    ) -&gt; pl.Expr:\n        \"\"\"\n        Returns whether the Levenshtein distance between self and other is &lt;= bound. This is much\n        faster than computing levenshtein distance and then doing &lt;= bound.\n\n        Parameters\n        ----------\n        other\n            If this is a string, then the entire column will be compared with this string. If this\n            is an expression, then an element-wise Levenshtein distance computation between this column\n            and the other (given by the expression) will be performed.\n        bound\n            Closed upper bound. If levenshtein distance &lt;= bound, return true and false otherwise.\n        parallel\n            Whether to run the comparisons in parallel. Note that this is not always faster, especially\n            when used with other expressions or in group_by/over context.\n        \"\"\"\n        if isinstance(other, str):\n            other_ = pl.lit(other, pl.Utf8)\n        else:\n            other_ = other\n\n        bound = pl.lit(abs(bound), pl.UInt32)\n        return self._expr.register_plugin(\n            lib=_lib,\n            symbol=\"pl_levenshtein_within\",\n            args=[other_, bound, pl.lit(parallel, pl.Boolean)],\n            is_elementwise=True,\n        )\n\n    def d_levenshtein(\n        self, other: Union[str, pl.Expr], parallel: bool = False, return_sim: bool = False\n    ) -&gt; pl.Expr:\n        \"\"\"\n        Computes the Damerau-Levenshtein distance between this and the other str.\n\n        Parameters\n        ----------\n        other\n            If this is a string, then the entire column will be compared with this string. If this\n            is an expression, then an element-wise Levenshtein distance computation between this column\n            and the other (given by the expression) will be performed.\n        parallel\n            Whether to run the comparisons in parallel. Note that this is not always faster, especially\n            when used with other expressions or in group_by/over context.\n        return_sim\n            If true, return normalized Damerau-Levenshtein.\n        \"\"\"\n        if isinstance(other, str):\n            other_ = pl.lit(other, dtype=pl.Utf8)\n        else:\n            other_ = other\n\n        if return_sim:\n            return self._expr.register_plugin(\n                lib=_lib,\n                symbol=\"pl_d_levenshtein_sim\",\n                args=[other_, pl.lit(parallel, pl.Boolean)],\n                is_elementwise=True,\n            )\n        else:\n            return self._expr.register_plugin(\n                lib=_lib,\n                symbol=\"pl_d_levenshtein\",\n                args=[other_, pl.lit(parallel, pl.Boolean)],\n                is_elementwise=True,\n            )\n\n    def osa(\n        self, other: Union[str, pl.Expr], parallel: bool = False, return_sim: bool = False\n    ) -&gt; pl.Expr:\n        \"\"\"\n        Computes the Optimal String Alignment distance between this and the other str.\n\n        Parameters\n        ----------\n        other\n            If this is a string, then the entire column will be compared with this string. If this\n            is an expression, then an element-wise OSA distance computation between this column\n            and the other (given by the expression) will be performed.\n        parallel\n            Whether to run the comparisons in parallel. Note that this is not always faster, especially\n            when used with other expressions or in group_by/over context.\n        return_sim\n            If true, return normalized OSA similarity.\n        \"\"\"\n        if isinstance(other, str):\n            other_ = pl.lit(other, dtype=pl.Utf8)\n        else:\n            other_ = other\n\n        if return_sim:\n            return self._expr.register_plugin(\n                lib=_lib,\n                symbol=\"pl_osa_sim\",\n                args=[other_, pl.lit(parallel, pl.Boolean)],\n                is_elementwise=True,\n            )\n        else:\n            return self._expr.register_plugin(\n                lib=_lib,\n                symbol=\"pl_osa\",\n                args=[other_, pl.lit(parallel, pl.Boolean)],\n                is_elementwise=True,\n            )\n\n    def jaro(self, other: Union[str, pl.Expr], parallel: bool = False) -&gt; pl.Expr:\n        \"\"\"\n        Computes the Jaro similarity between this and the other str. Jaro distance = 1 - Jaro sim.\n\n        Parameters\n        ----------\n        other\n            If this is a string, then the entire column will be compared with this string. If this\n            is an expression, then an element-wise Levenshtein distance computation between this column\n            and the other (given by the expression) will be performed.\n        parallel\n            Whether to run the comparisons in parallel. Note that this is not always faster, especially\n            when used with other expressions or in group_by/over context.\n        \"\"\"\n        if isinstance(other, str):\n            other_ = pl.lit(other, dtype=pl.Utf8)\n        else:\n            other_ = other\n\n        return self._expr.register_plugin(\n            lib=_lib,\n            symbol=\"pl_jaro\",\n            args=[other_, pl.lit(parallel, pl.Boolean)],\n            is_elementwise=True,\n        )\n\n    def jw(\n        self, other: Union[str, pl.Expr], weight: float = 0.1, parallel: bool = False\n    ) -&gt; pl.Expr:\n        \"\"\"\n        Computes the Jaro-Winker similarity between this and the other str.\n        Jaro-Winkler distance = 1 - Jaro-Winkler sim.\n\n        Parameters\n        ----------\n        other\n            If this is a string, then the entire column will be compared with this string. If this\n            is an expression, then an element-wise Levenshtein distance computation between this column\n            and the other (given by the expression) will be performed.\n        weight\n            Weight for prefix. A typical value is 0.1.\n        parallel\n            Whether to run the comparisons in parallel. Note that this is not always faster, especially\n            when used with other expressions or in group_by/over context.\n        \"\"\"\n        if isinstance(other, str):\n            other_ = pl.lit(other, pl.Utf8)\n        else:\n            other_ = other\n\n        return self._expr.register_plugin(\n            lib=_lib,\n            symbol=\"pl_jw\",\n            args=[other_, pl.lit(weight, pl.Float64), pl.lit(parallel, pl.Boolean)],\n            is_elementwise=True,\n        )\n\n    def hamming(\n        self, other: Union[str, pl.Expr], pad: bool = False, parallel: bool = False\n    ) -&gt; pl.Expr:\n        \"\"\"\n        Computes the hamming distance between two strings. If they do not have the same length, null will\n        be returned.\n\n        Parameters\n        ----------\n        other\n            If this is a string, then the entire column will be compared with this string. If this\n            is an expression, then an element-wise hamming distance computation between this column\n            and the other (given by the expression) will be performed.\n        pad\n            Whether to pad the string when lengths are not equal.\n        parallel\n            Whether to run the comparisons in parallel. Note that this is not always faster, especially\n            when used with other expressions or in group_by/over context.\n        \"\"\"\n        if isinstance(other, str):\n            other_ = pl.lit(other)\n        else:\n            other_ = other\n\n        return self._expr.register_plugin(\n            lib=_lib,\n            symbol=\"pl_hamming\",\n            args=[other_, pl.lit(pad, pl.Boolean), pl.lit(parallel, pl.Boolean)],\n            is_elementwise=True,\n        )\n\n    def tokenize(self, pattern: str = r\"(?u)\\b\\w\\w+\\b\", stem: bool = False) -&gt; pl.Expr:\n        \"\"\"\n        Tokenize the string according to the pattern. This will only extract the words\n        satisfying the pattern.\n\n        Parameters\n        ----------\n        pattern\n            The word pattern to extract\n        stem\n            If true, then this will stem the words and keep only the unique ones. Stop words\n            will be removed. (Common words like `he`, `she`, etc., will be removed.)\n        \"\"\"\n        out = self._expr.str.extract_all(pattern)\n        if stem:\n            out = out.list.eval(\n                pl.element()\n                .register_plugin(\n                    lib=_lib,\n                    symbol=\"pl_snowball_stem\",\n                    args=[pl.lit(True, pl.Boolean), pl.lit(False, pl.Boolean)],\n                    is_elementwise=True,\n                )  # True to no stop word, False to Parallel\n                .drop_nulls()\n            )\n        return out\n\n    def freq_removal(\n        self, lower: float = 0.05, upper: float = 0.95, parallel: bool = True\n    ) -&gt; pl.Expr:\n        \"\"\"\n        Removes from each documents words that are too frequent (in the entire dataset). This assumes\n        that the input expression represents lists of strings. E.g. output of tokenize.\n\n        Parameters\n        ----------\n        lower\n            Lower percentile. If a word's frequency is &lt; than this, it will be removed.\n        upper\n            Upper percentile. If a word's frequency is &gt; than this, it will be removed.\n        parallel\n            Whether to run word count in parallel. It is not recommended when you are in a group_by\n            context.\n        \"\"\"\n\n        name = self._expr.meta.root_names()[0]\n        vc = self._expr.list.explode().value_counts(parallel=parallel).sort()\n        lo = vc.struct.field(\"count\").quantile(lower)\n        u = vc.struct.field(\"count\").quantile(upper)\n        remove = (\n            vc.filter((vc.struct.field(\"count\") &lt; lo) | (vc.struct.field(\"count\") &gt; u))\n            .struct.field(name)\n            .implode()\n        )\n\n        return self._expr.list.set_difference(remove)\n\n    def snowball(self, no_stopwords: bool = True, parallel: bool = False) -&gt; pl.Expr:\n        \"\"\"\n        Applies the snowball stemmer for the column. The column is supposed to be a column of single words.\n\n        Parameters\n        ----------\n        no_stopwords\n            If true, stopwords will be mapped to None. If false, stopwords will be stemmed.\n        parallel\n            Whether to run the comparisons in parallel. Note that this is not always faster, especially\n            when used with other expressions or in group_by/over context.\n        \"\"\"\n        return self._expr.register_plugin(\n            lib=_lib,\n            symbol=\"pl_snowball_stem\",\n            args=[pl.lit(no_stopwords, pl.Boolean), pl.lit(parallel, pl.Boolean)],\n            is_elementwise=True,\n        )\n\n    def ac_match(\n        self,\n        patterns: list[str],\n        case_sensitive: bool = False,\n        match_kind: AhoCorasickMatchKind = \"standard\",\n        return_str: bool = False,\n    ) -&gt; pl.Expr:\n        \"\"\"\n        Try to match the patterns using the Aho-Corasick algorithm. The matched pattern's indices will be\n        returned. E.g. If for string1, pattern 2, 1, 3 are matched in this order, then [1, 0, 2] are\n        returned. (Indices in pattern list)\n\n        Polars &gt;= 0.20 now has native aho-corasick support. The backend package is the same, though the function\n        api is different. See polars's str.contains_any and str.replace_many.\n\n        Parameters\n        ----------\n        patterns\n            A list of strs, which are patterns to be matched\n        case_sensitive\n            Should this match be case sensitive? Default is false. Not working now.\n        match_kind\n            One of `standard`, `left_most_first`, or `left_most_longest`. For more information, see\n            https://docs.rs/aho-corasick/latest/aho_corasick/enum.MatchKind.html. Any other input will\n            be treated as standard.\n        \"\"\"\n\n        # Currently value_capacity for each list is hard-coded to 20. If there are more than 20 matches,\n        # then this will be slow (doubling vec capacity)\n        warnings.warn(\"Argument `case_sensitive` does not seem to work right now.\")\n        warnings.warn(\n            \"This function is unstable and is subject to change and may not perform well if there are more than \"\n            \"20 matches. Read the source code or contact the author for more information. The most difficulty part \"\n            \"is to design an output API that works well with Polars, which is harder than one might think.\"\n        )\n\n        pat = pl.Series(patterns, dtype=pl.Utf8)\n        cs = pl.lit(case_sensitive, pl.Boolean)\n        mk = pl.lit(match_kind, pl.Utf8)\n        if return_str:\n            return self._expr.register_plugin(\n                lib=_lib,\n                symbol=\"pl_ac_match_str\",\n                args=[pat, cs, mk],\n                is_elementwise=True,\n            )\n        else:\n            return self._expr.register_plugin(\n                lib=_lib,\n                symbol=\"pl_ac_match\",\n                args=[pat, cs, mk],\n                is_elementwise=True,\n            )\n\n    def ac_replace(\n        self, patterns: list[str], replacements: list[str], parallel: bool = False\n    ) -&gt; pl.Expr:\n        \"\"\"\n        Try to replace the patterns using the Aho-Corasick algorithm. The length of patterns should match\n        the length of replacements. If not, both sequences will be capped at the shorter length. If an error\n        happens during replacement, None will be returned.\n\n        Polars &gt;= 0.20 now has native aho-corasick support. The backend package is the same, though the function\n        api is different. See polars's str.contains_any and str.replace_many.\n\n        Parameters\n        ----------\n        patterns\n            A list of strs, which are patterns to be matched\n        replacements\n            A list of strs to replace the patterns with\n        parallel\n            Whether to run the comparisons in parallel. Note that this is not always faster, especially\n            when used with other expressions or in group_by/over context.\n        \"\"\"\n        if (len(replacements) == 0) or (len(patterns) == 0):\n            return self._expr\n\n        mlen = min(len(patterns), len(replacements))\n        pat = pl.Series(patterns[:mlen], dtype=pl.Utf8)\n        rpl = pl.Series(replacements[:mlen], dtype=pl.Utf8)\n        par = pl.lit(parallel, pl.Boolean)\n        return self._expr.register_plugin(\n            lib=_lib,\n            symbol=\"pl_ac_replace\",\n            args=[pat, rpl, par],\n            is_elementwise=True,\n        )\n</code></pre>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.ac_match","title":"<code>ac_match(patterns, case_sensitive=False, match_kind='standard', return_str=False)</code>","text":"<p>Try to match the patterns using the Aho-Corasick algorithm. The matched pattern's indices will be returned. E.g. If for string1, pattern 2, 1, 3 are matched in this order, then [1, 0, 2] are returned. (Indices in pattern list)</p> <p>Polars &gt;= 0.20 now has native aho-corasick support. The backend package is the same, though the function api is different. See polars's str.contains_any and str.replace_many.</p>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.ac_match--parameters","title":"Parameters","text":"<p>patterns     A list of strs, which are patterns to be matched case_sensitive     Should this match be case sensitive? Default is false. Not working now. match_kind     One of <code>standard</code>, <code>left_most_first</code>, or <code>left_most_longest</code>. For more information, see     https://docs.rs/aho-corasick/latest/aho_corasick/enum.MatchKind.html. Any other input will     be treated as standard.</p> Source code in <code>python/polars_ds/str_ext.py</code> <pre><code>def ac_match(\n    self,\n    patterns: list[str],\n    case_sensitive: bool = False,\n    match_kind: AhoCorasickMatchKind = \"standard\",\n    return_str: bool = False,\n) -&gt; pl.Expr:\n    \"\"\"\n    Try to match the patterns using the Aho-Corasick algorithm. The matched pattern's indices will be\n    returned. E.g. If for string1, pattern 2, 1, 3 are matched in this order, then [1, 0, 2] are\n    returned. (Indices in pattern list)\n\n    Polars &gt;= 0.20 now has native aho-corasick support. The backend package is the same, though the function\n    api is different. See polars's str.contains_any and str.replace_many.\n\n    Parameters\n    ----------\n    patterns\n        A list of strs, which are patterns to be matched\n    case_sensitive\n        Should this match be case sensitive? Default is false. Not working now.\n    match_kind\n        One of `standard`, `left_most_first`, or `left_most_longest`. For more information, see\n        https://docs.rs/aho-corasick/latest/aho_corasick/enum.MatchKind.html. Any other input will\n        be treated as standard.\n    \"\"\"\n\n    # Currently value_capacity for each list is hard-coded to 20. If there are more than 20 matches,\n    # then this will be slow (doubling vec capacity)\n    warnings.warn(\"Argument `case_sensitive` does not seem to work right now.\")\n    warnings.warn(\n        \"This function is unstable and is subject to change and may not perform well if there are more than \"\n        \"20 matches. Read the source code or contact the author for more information. The most difficulty part \"\n        \"is to design an output API that works well with Polars, which is harder than one might think.\"\n    )\n\n    pat = pl.Series(patterns, dtype=pl.Utf8)\n    cs = pl.lit(case_sensitive, pl.Boolean)\n    mk = pl.lit(match_kind, pl.Utf8)\n    if return_str:\n        return self._expr.register_plugin(\n            lib=_lib,\n            symbol=\"pl_ac_match_str\",\n            args=[pat, cs, mk],\n            is_elementwise=True,\n        )\n    else:\n        return self._expr.register_plugin(\n            lib=_lib,\n            symbol=\"pl_ac_match\",\n            args=[pat, cs, mk],\n            is_elementwise=True,\n        )\n</code></pre>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.ac_replace","title":"<code>ac_replace(patterns, replacements, parallel=False)</code>","text":"<p>Try to replace the patterns using the Aho-Corasick algorithm. The length of patterns should match the length of replacements. If not, both sequences will be capped at the shorter length. If an error happens during replacement, None will be returned.</p> <p>Polars &gt;= 0.20 now has native aho-corasick support. The backend package is the same, though the function api is different. See polars's str.contains_any and str.replace_many.</p>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.ac_replace--parameters","title":"Parameters","text":"<p>patterns     A list of strs, which are patterns to be matched replacements     A list of strs to replace the patterns with parallel     Whether to run the comparisons in parallel. Note that this is not always faster, especially     when used with other expressions or in group_by/over context.</p> Source code in <code>python/polars_ds/str_ext.py</code> <pre><code>def ac_replace(\n    self, patterns: list[str], replacements: list[str], parallel: bool = False\n) -&gt; pl.Expr:\n    \"\"\"\n    Try to replace the patterns using the Aho-Corasick algorithm. The length of patterns should match\n    the length of replacements. If not, both sequences will be capped at the shorter length. If an error\n    happens during replacement, None will be returned.\n\n    Polars &gt;= 0.20 now has native aho-corasick support. The backend package is the same, though the function\n    api is different. See polars's str.contains_any and str.replace_many.\n\n    Parameters\n    ----------\n    patterns\n        A list of strs, which are patterns to be matched\n    replacements\n        A list of strs to replace the patterns with\n    parallel\n        Whether to run the comparisons in parallel. Note that this is not always faster, especially\n        when used with other expressions or in group_by/over context.\n    \"\"\"\n    if (len(replacements) == 0) or (len(patterns) == 0):\n        return self._expr\n\n    mlen = min(len(patterns), len(replacements))\n    pat = pl.Series(patterns[:mlen], dtype=pl.Utf8)\n    rpl = pl.Series(replacements[:mlen], dtype=pl.Utf8)\n    par = pl.lit(parallel, pl.Boolean)\n    return self._expr.register_plugin(\n        lib=_lib,\n        symbol=\"pl_ac_replace\",\n        args=[pat, rpl, par],\n        is_elementwise=True,\n    )\n</code></pre>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.d_levenshtein","title":"<code>d_levenshtein(other, parallel=False, return_sim=False)</code>","text":"<p>Computes the Damerau-Levenshtein distance between this and the other str.</p>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.d_levenshtein--parameters","title":"Parameters","text":"<p>other     If this is a string, then the entire column will be compared with this string. If this     is an expression, then an element-wise Levenshtein distance computation between this column     and the other (given by the expression) will be performed. parallel     Whether to run the comparisons in parallel. Note that this is not always faster, especially     when used with other expressions or in group_by/over context. return_sim     If true, return normalized Damerau-Levenshtein.</p> Source code in <code>python/polars_ds/str_ext.py</code> <pre><code>def d_levenshtein(\n    self, other: Union[str, pl.Expr], parallel: bool = False, return_sim: bool = False\n) -&gt; pl.Expr:\n    \"\"\"\n    Computes the Damerau-Levenshtein distance between this and the other str.\n\n    Parameters\n    ----------\n    other\n        If this is a string, then the entire column will be compared with this string. If this\n        is an expression, then an element-wise Levenshtein distance computation between this column\n        and the other (given by the expression) will be performed.\n    parallel\n        Whether to run the comparisons in parallel. Note that this is not always faster, especially\n        when used with other expressions or in group_by/over context.\n    return_sim\n        If true, return normalized Damerau-Levenshtein.\n    \"\"\"\n    if isinstance(other, str):\n        other_ = pl.lit(other, dtype=pl.Utf8)\n    else:\n        other_ = other\n\n    if return_sim:\n        return self._expr.register_plugin(\n            lib=_lib,\n            symbol=\"pl_d_levenshtein_sim\",\n            args=[other_, pl.lit(parallel, pl.Boolean)],\n            is_elementwise=True,\n        )\n    else:\n        return self._expr.register_plugin(\n            lib=_lib,\n            symbol=\"pl_d_levenshtein\",\n            args=[other_, pl.lit(parallel, pl.Boolean)],\n            is_elementwise=True,\n        )\n</code></pre>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.extract_numbers","title":"<code>extract_numbers(ignore_comma=False, join_by='', dtype=pl.Utf8)</code>","text":"<p>Extracts numbers from the string column, and stores them in a list.</p>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.extract_numbers--parameters","title":"Parameters","text":"<p>ignore_comma     Whether to remove all comma before matching for numbers join_by     If dtype is pl.Utf8, join the list of strings using the value given here dtype     The desired inner dtype for the extracted data. Should either be one of     pl.NUMERIC_DTYPES or pl.Utf8</p>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.extract_numbers--examples","title":"Examples","text":"<p>df = pl.DataFrame({ ...     \"survey\":[\"0% of my time\", \"1% to 25% of my time\", \"75% to 99% of my time\", ...            \"50% to 74% of my time\", \"75% to 99% of my time\", ...            \"50% to 74% of my time\"] ... }) df.select(pl.col(\"survey\").str_ext.extract_numbers(dtype=pl.UInt32)) shape: (6, 1) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 survey    \u2502 \u2502 ---       \u2502 \u2502 list[u32] \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 [0]       \u2502 \u2502 [1, 25]   \u2502 \u2502 [75, 99]  \u2502 \u2502 [50, 74]  \u2502 \u2502 [75, 99]  \u2502 \u2502 [50, 74]  \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 df.select(pl.col(\"survey\").str_ext.extract_numbers(join_by=\"-\", dtype=pl.Utf8)) shape: (6, 1) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 survey \u2502 \u2502 ---    \u2502 \u2502 str    \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 0      \u2502 \u2502 1-25   \u2502 \u2502 75-99  \u2502 \u2502 50-74  \u2502 \u2502 75-99  \u2502 \u2502 50-74  \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</p> Source code in <code>python/polars_ds/str_ext.py</code> <pre><code>def extract_numbers(\n    self, ignore_comma: bool = False, join_by: str = \"\", dtype: pl.DataType = pl.Utf8\n) -&gt; pl.Expr:\n    \"\"\"\n    Extracts numbers from the string column, and stores them in a list.\n\n    Parameters\n    ----------\n    ignore_comma\n        Whether to remove all comma before matching for numbers\n    join_by\n        If dtype is pl.Utf8, join the list of strings using the value given here\n    dtype\n        The desired inner dtype for the extracted data. Should either be one of\n        pl.NUMERIC_DTYPES or pl.Utf8\n\n    Examples\n    --------\n    &gt;&gt;&gt; df = pl.DataFrame({\n    ...     \"survey\":[\"0% of my time\", \"1% to 25% of my time\", \"75% to 99% of my time\",\n    ...            \"50% to 74% of my time\", \"75% to 99% of my time\",\n    ...            \"50% to 74% of my time\"]\n    ... })\n    &gt;&gt;&gt; df.select(pl.col(\"survey\").str_ext.extract_numbers(dtype=pl.UInt32))\n    shape: (6, 1)\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 survey    \u2502\n    \u2502 ---       \u2502\n    \u2502 list[u32] \u2502\n    \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n    \u2502 [0]       \u2502\n    \u2502 [1, 25]   \u2502\n    \u2502 [75, 99]  \u2502\n    \u2502 [50, 74]  \u2502\n    \u2502 [75, 99]  \u2502\n    \u2502 [50, 74]  \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    &gt;&gt;&gt; df.select(pl.col(\"survey\").str_ext.extract_numbers(join_by=\"-\", dtype=pl.Utf8))\n    shape: (6, 1)\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 survey \u2502\n    \u2502 ---    \u2502\n    \u2502 str    \u2502\n    \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n    \u2502 0      \u2502\n    \u2502 1-25   \u2502\n    \u2502 75-99  \u2502\n    \u2502 50-74  \u2502\n    \u2502 75-99  \u2502\n    \u2502 50-74  \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \"\"\"\n    expr = self._expr\n    if ignore_comma:\n        expr = expr.str.replace_all(\",\", \"\")\n\n    # Find all numbers\n    expr = expr.str.extract_all(\"(\\d*\\.?\\d+)\")\n    if dtype in pl.NUMERIC_DTYPES:\n        expr = expr.list.eval(pl.element().cast(dtype))\n    elif dtype == pl.Utf8:  # As a list of strings\n        if join_by != \"\":\n            expr = expr.list.join(join_by)\n\n    return expr\n</code></pre>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.freq_removal","title":"<code>freq_removal(lower=0.05, upper=0.95, parallel=True)</code>","text":"<p>Removes from each documents words that are too frequent (in the entire dataset). This assumes that the input expression represents lists of strings. E.g. output of tokenize.</p>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.freq_removal--parameters","title":"Parameters","text":"<p>lower     Lower percentile. If a word's frequency is &lt; than this, it will be removed. upper     Upper percentile. If a word's frequency is &gt; than this, it will be removed. parallel     Whether to run word count in parallel. It is not recommended when you are in a group_by     context.</p> Source code in <code>python/polars_ds/str_ext.py</code> <pre><code>def freq_removal(\n    self, lower: float = 0.05, upper: float = 0.95, parallel: bool = True\n) -&gt; pl.Expr:\n    \"\"\"\n    Removes from each documents words that are too frequent (in the entire dataset). This assumes\n    that the input expression represents lists of strings. E.g. output of tokenize.\n\n    Parameters\n    ----------\n    lower\n        Lower percentile. If a word's frequency is &lt; than this, it will be removed.\n    upper\n        Upper percentile. If a word's frequency is &gt; than this, it will be removed.\n    parallel\n        Whether to run word count in parallel. It is not recommended when you are in a group_by\n        context.\n    \"\"\"\n\n    name = self._expr.meta.root_names()[0]\n    vc = self._expr.list.explode().value_counts(parallel=parallel).sort()\n    lo = vc.struct.field(\"count\").quantile(lower)\n    u = vc.struct.field(\"count\").quantile(upper)\n    remove = (\n        vc.filter((vc.struct.field(\"count\") &lt; lo) | (vc.struct.field(\"count\") &gt; u))\n        .struct.field(name)\n        .implode()\n    )\n\n    return self._expr.list.set_difference(remove)\n</code></pre>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.hamming","title":"<code>hamming(other, pad=False, parallel=False)</code>","text":"<p>Computes the hamming distance between two strings. If they do not have the same length, null will be returned.</p>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.hamming--parameters","title":"Parameters","text":"<p>other     If this is a string, then the entire column will be compared with this string. If this     is an expression, then an element-wise hamming distance computation between this column     and the other (given by the expression) will be performed. pad     Whether to pad the string when lengths are not equal. parallel     Whether to run the comparisons in parallel. Note that this is not always faster, especially     when used with other expressions or in group_by/over context.</p> Source code in <code>python/polars_ds/str_ext.py</code> <pre><code>def hamming(\n    self, other: Union[str, pl.Expr], pad: bool = False, parallel: bool = False\n) -&gt; pl.Expr:\n    \"\"\"\n    Computes the hamming distance between two strings. If they do not have the same length, null will\n    be returned.\n\n    Parameters\n    ----------\n    other\n        If this is a string, then the entire column will be compared with this string. If this\n        is an expression, then an element-wise hamming distance computation between this column\n        and the other (given by the expression) will be performed.\n    pad\n        Whether to pad the string when lengths are not equal.\n    parallel\n        Whether to run the comparisons in parallel. Note that this is not always faster, especially\n        when used with other expressions or in group_by/over context.\n    \"\"\"\n    if isinstance(other, str):\n        other_ = pl.lit(other)\n    else:\n        other_ = other\n\n    return self._expr.register_plugin(\n        lib=_lib,\n        symbol=\"pl_hamming\",\n        args=[other_, pl.lit(pad, pl.Boolean), pl.lit(parallel, pl.Boolean)],\n        is_elementwise=True,\n    )\n</code></pre>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.infer_infreq","title":"<code>infer_infreq(*, min_count=None, min_frac=None, parallel=False)</code>","text":"<p>Infers infrequent categories (strings) by min_count or min_frac and return a list as output.</p>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.infer_infreq--parameters","title":"Parameters","text":"<p>min_count     If set, an infrequency category will be defined as a category with count &lt; this. min_frac     If set, an infrequency category will be defined as a category with pct &lt; this. min_count     takes priority over this. parallel     Whether to run value_counts in parallel. This may not provide much speed up and is not     recommended in a group_by context.</p> Source code in <code>python/polars_ds/str_ext.py</code> <pre><code>def infer_infreq(\n    self,\n    *,\n    min_count: Optional[int] = None,\n    min_frac: Optional[float] = None,\n    parallel: bool = False,\n) -&gt; pl.Expr:\n    \"\"\"\n    Infers infrequent categories (strings) by min_count or min_frac and return a list as output.\n\n    Parameters\n    ----------\n    min_count\n        If set, an infrequency category will be defined as a category with count &lt; this.\n    min_frac\n        If set, an infrequency category will be defined as a category with pct &lt; this. min_count\n        takes priority over this.\n    parallel\n        Whether to run value_counts in parallel. This may not provide much speed up and is not\n        recommended in a group_by context.\n    \"\"\"\n    name = self._expr.meta.root_names()[0]\n    vc = self._expr.value_counts(parallel=parallel, sort=True)\n    if min_count is None and min_frac is None:\n        raise ValueError(\"Either min_count or min_frac must be provided.\")\n    elif min_count is not None:\n        infreq: pl.Expr = vc.filter(vc.struct.field(\"count\") &lt; min_count).struct.field(name)\n    elif min_frac is not None:\n        infreq: pl.Expr = vc.filter(\n            vc.struct.field(\"count\") / vc.struct.field(\"count\").sum() &lt; min_frac\n        ).struct.field(name)\n\n    return infreq.implode()\n</code></pre>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.is_stopword","title":"<code>is_stopword()</code>","text":"<p>Checks whether the string is a stopword or not.</p> Source code in <code>python/polars_ds/str_ext.py</code> <pre><code>def is_stopword(self) -&gt; pl.Expr:\n    \"\"\"\n    Checks whether the string is a stopword or not.\n    \"\"\"\n    self._expr.register_plugin(\n        lib=_lib,\n        symbol=\"pl_is_stopword\",\n        args=[],\n        is_elementwise=True,\n    )\n</code></pre>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.jaro","title":"<code>jaro(other, parallel=False)</code>","text":"<p>Computes the Jaro similarity between this and the other str. Jaro distance = 1 - Jaro sim.</p>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.jaro--parameters","title":"Parameters","text":"<p>other     If this is a string, then the entire column will be compared with this string. If this     is an expression, then an element-wise Levenshtein distance computation between this column     and the other (given by the expression) will be performed. parallel     Whether to run the comparisons in parallel. Note that this is not always faster, especially     when used with other expressions or in group_by/over context.</p> Source code in <code>python/polars_ds/str_ext.py</code> <pre><code>def jaro(self, other: Union[str, pl.Expr], parallel: bool = False) -&gt; pl.Expr:\n    \"\"\"\n    Computes the Jaro similarity between this and the other str. Jaro distance = 1 - Jaro sim.\n\n    Parameters\n    ----------\n    other\n        If this is a string, then the entire column will be compared with this string. If this\n        is an expression, then an element-wise Levenshtein distance computation between this column\n        and the other (given by the expression) will be performed.\n    parallel\n        Whether to run the comparisons in parallel. Note that this is not always faster, especially\n        when used with other expressions or in group_by/over context.\n    \"\"\"\n    if isinstance(other, str):\n        other_ = pl.lit(other, dtype=pl.Utf8)\n    else:\n        other_ = other\n\n    return self._expr.register_plugin(\n        lib=_lib,\n        symbol=\"pl_jaro\",\n        args=[other_, pl.lit(parallel, pl.Boolean)],\n        is_elementwise=True,\n    )\n</code></pre>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.jw","title":"<code>jw(other, weight=0.1, parallel=False)</code>","text":"<p>Computes the Jaro-Winker similarity between this and the other str. Jaro-Winkler distance = 1 - Jaro-Winkler sim.</p>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.jw--parameters","title":"Parameters","text":"<p>other     If this is a string, then the entire column will be compared with this string. If this     is an expression, then an element-wise Levenshtein distance computation between this column     and the other (given by the expression) will be performed. weight     Weight for prefix. A typical value is 0.1. parallel     Whether to run the comparisons in parallel. Note that this is not always faster, especially     when used with other expressions or in group_by/over context.</p> Source code in <code>python/polars_ds/str_ext.py</code> <pre><code>def jw(\n    self, other: Union[str, pl.Expr], weight: float = 0.1, parallel: bool = False\n) -&gt; pl.Expr:\n    \"\"\"\n    Computes the Jaro-Winker similarity between this and the other str.\n    Jaro-Winkler distance = 1 - Jaro-Winkler sim.\n\n    Parameters\n    ----------\n    other\n        If this is a string, then the entire column will be compared with this string. If this\n        is an expression, then an element-wise Levenshtein distance computation between this column\n        and the other (given by the expression) will be performed.\n    weight\n        Weight for prefix. A typical value is 0.1.\n    parallel\n        Whether to run the comparisons in parallel. Note that this is not always faster, especially\n        when used with other expressions or in group_by/over context.\n    \"\"\"\n    if isinstance(other, str):\n        other_ = pl.lit(other, pl.Utf8)\n    else:\n        other_ = other\n\n    return self._expr.register_plugin(\n        lib=_lib,\n        symbol=\"pl_jw\",\n        args=[other_, pl.lit(weight, pl.Float64), pl.lit(parallel, pl.Boolean)],\n        is_elementwise=True,\n    )\n</code></pre>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.levenshtein","title":"<code>levenshtein(other, parallel=False, return_sim=False)</code>","text":"<p>Computes the Levenshtein distance between this and the other str.</p>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.levenshtein--parameters","title":"Parameters","text":"<p>other     If this is a string, then the entire column will be compared with this string. If this     is an expression, then an element-wise Levenshtein distance computation between this column     and the other (given by the expression) will be performed. parallel     Whether to run the comparisons in parallel. Note that this is not always faster, especially     when used with other expressions or in group_by/over context. return_sim     If true, return normalized Levenshtein.</p> Source code in <code>python/polars_ds/str_ext.py</code> <pre><code>def levenshtein(\n    self, other: Union[str, pl.Expr], parallel: bool = False, return_sim: bool = False\n) -&gt; pl.Expr:\n    \"\"\"\n    Computes the Levenshtein distance between this and the other str.\n\n    Parameters\n    ----------\n    other\n        If this is a string, then the entire column will be compared with this string. If this\n        is an expression, then an element-wise Levenshtein distance computation between this column\n        and the other (given by the expression) will be performed.\n    parallel\n        Whether to run the comparisons in parallel. Note that this is not always faster, especially\n        when used with other expressions or in group_by/over context.\n    return_sim\n        If true, return normalized Levenshtein.\n    \"\"\"\n    if isinstance(other, str):\n        other_ = pl.lit(other, dtype=pl.Utf8)\n    else:\n        other_ = other\n\n    if return_sim:\n        return self._expr.register_plugin(\n            lib=_lib,\n            symbol=\"pl_levenshtein_sim\",\n            args=[other_, pl.lit(parallel, pl.Boolean)],\n            is_elementwise=True,\n        )\n    else:\n        return self._expr.register_plugin(\n            lib=_lib,\n            symbol=\"pl_levenshtein\",\n            args=[other_, pl.lit(parallel, pl.Boolean)],\n            is_elementwise=True,\n        )\n</code></pre>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.levenshtein_within","title":"<code>levenshtein_within(other, bound, parallel=False)</code>","text":"<p>Returns whether the Levenshtein distance between self and other is &lt;= bound. This is much faster than computing levenshtein distance and then doing &lt;= bound.</p>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.levenshtein_within--parameters","title":"Parameters","text":"<p>other     If this is a string, then the entire column will be compared with this string. If this     is an expression, then an element-wise Levenshtein distance computation between this column     and the other (given by the expression) will be performed. bound     Closed upper bound. If levenshtein distance &lt;= bound, return true and false otherwise. parallel     Whether to run the comparisons in parallel. Note that this is not always faster, especially     when used with other expressions or in group_by/over context.</p> Source code in <code>python/polars_ds/str_ext.py</code> <pre><code>def levenshtein_within(\n    self,\n    other: Union[str, pl.Expr],\n    bound: int,\n    parallel: bool = False,\n) -&gt; pl.Expr:\n    \"\"\"\n    Returns whether the Levenshtein distance between self and other is &lt;= bound. This is much\n    faster than computing levenshtein distance and then doing &lt;= bound.\n\n    Parameters\n    ----------\n    other\n        If this is a string, then the entire column will be compared with this string. If this\n        is an expression, then an element-wise Levenshtein distance computation between this column\n        and the other (given by the expression) will be performed.\n    bound\n        Closed upper bound. If levenshtein distance &lt;= bound, return true and false otherwise.\n    parallel\n        Whether to run the comparisons in parallel. Note that this is not always faster, especially\n        when used with other expressions or in group_by/over context.\n    \"\"\"\n    if isinstance(other, str):\n        other_ = pl.lit(other, pl.Utf8)\n    else:\n        other_ = other\n\n    bound = pl.lit(abs(bound), pl.UInt32)\n    return self._expr.register_plugin(\n        lib=_lib,\n        symbol=\"pl_levenshtein_within\",\n        args=[other_, bound, pl.lit(parallel, pl.Boolean)],\n        is_elementwise=True,\n    )\n</code></pre>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.line_count","title":"<code>line_count()</code>","text":"<p>Return the line count of the string column.</p> Source code in <code>python/polars_ds/str_ext.py</code> <pre><code>def line_count(self) -&gt; pl.Expr:\n    \"\"\"\n    Return the line count of the string column.\n    \"\"\"\n    return self._expr.str.count_matches(pattern=\"\\n\")\n</code></pre>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.merge_infreq","title":"<code>merge_infreq(*, min_count=None, min_frac=None, separator='|', parallel=False)</code>","text":"<p>Merge infrequent categories (strings) in the column into one category (string) separated by a separator. This is useful when you want to do one-hot-encoding but do not want too many distinct values because of low count values. However, this does not mean that the categories are similar with respect to the your modelling problem.</p>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.merge_infreq--parameters","title":"Parameters","text":"<p>min_count     If set, an infrequency category will be defined as a category with count &lt; this. min_frac     If set, an infrequency category will be defined as a category with pct &lt; this. min_count     takes priority over this. separator     What separator to use when joining the categories. E.g if \"a\" and \"b\" are rare categories,     and separator = \"|\", they will be mapped to \"a|b\" parallel     Whether to run value_counts in parallel. This may not provide much speed up and is not     recommended in a group_by context.</p> Source code in <code>python/polars_ds/str_ext.py</code> <pre><code>def merge_infreq(\n    self,\n    *,\n    min_count: Optional[int] = None,\n    min_frac: Optional[float] = None,\n    separator: str = \"|\",\n    parallel: bool = False,\n) -&gt; pl.Expr:\n    \"\"\"\n    Merge infrequent categories (strings) in the column into one category (string) separated by a\n    separator. This is useful when you want to do one-hot-encoding but do not want too many distinct\n    values because of low count values. However, this does not mean that the categories are similar\n    with respect to the your modelling problem.\n\n    Parameters\n    ----------\n    min_count\n        If set, an infrequency category will be defined as a category with count &lt; this.\n    min_frac\n        If set, an infrequency category will be defined as a category with pct &lt; this. min_count\n        takes priority over this.\n    separator\n        What separator to use when joining the categories. E.g if \"a\" and \"b\" are rare categories,\n        and separator = \"|\", they will be mapped to \"a|b\"\n    parallel\n        Whether to run value_counts in parallel. This may not provide much speed up and is not\n        recommended in a group_by context.\n    \"\"\"\n\n    # Will be fixed soon and sort will not be needed\n    name = self._expr.meta.root_names()[0]\n    vc = self._expr.value_counts(parallel=parallel, sort=True)\n    if min_count is None and min_frac is None:\n        raise ValueError(\"Either min_count or min_frac must be provided.\")\n    elif min_count is not None:\n        to_merge: pl.Expr = vc.filter(vc.struct.field(\"count\") &lt; min_count).struct.field(name)\n    elif min_frac is not None:\n        to_merge: pl.Expr = vc.filter(\n            vc.struct.field(\"count\") / vc.struct.field(\"count\").sum() &lt; min_frac\n        ).struct.field(name)\n\n    return (\n        pl.when(self._expr.is_in(to_merge))\n        .then(to_merge.cast(pl.Utf8).fill_null(\"null\").implode().first().list.join(separator))\n        .otherwise(self._expr)\n    )\n</code></pre>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.osa","title":"<code>osa(other, parallel=False, return_sim=False)</code>","text":"<p>Computes the Optimal String Alignment distance between this and the other str.</p>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.osa--parameters","title":"Parameters","text":"<p>other     If this is a string, then the entire column will be compared with this string. If this     is an expression, then an element-wise OSA distance computation between this column     and the other (given by the expression) will be performed. parallel     Whether to run the comparisons in parallel. Note that this is not always faster, especially     when used with other expressions or in group_by/over context. return_sim     If true, return normalized OSA similarity.</p> Source code in <code>python/polars_ds/str_ext.py</code> <pre><code>def osa(\n    self, other: Union[str, pl.Expr], parallel: bool = False, return_sim: bool = False\n) -&gt; pl.Expr:\n    \"\"\"\n    Computes the Optimal String Alignment distance between this and the other str.\n\n    Parameters\n    ----------\n    other\n        If this is a string, then the entire column will be compared with this string. If this\n        is an expression, then an element-wise OSA distance computation between this column\n        and the other (given by the expression) will be performed.\n    parallel\n        Whether to run the comparisons in parallel. Note that this is not always faster, especially\n        when used with other expressions or in group_by/over context.\n    return_sim\n        If true, return normalized OSA similarity.\n    \"\"\"\n    if isinstance(other, str):\n        other_ = pl.lit(other, dtype=pl.Utf8)\n    else:\n        other_ = other\n\n    if return_sim:\n        return self._expr.register_plugin(\n            lib=_lib,\n            symbol=\"pl_osa_sim\",\n            args=[other_, pl.lit(parallel, pl.Boolean)],\n            is_elementwise=True,\n        )\n    else:\n        return self._expr.register_plugin(\n            lib=_lib,\n            symbol=\"pl_osa\",\n            args=[other_, pl.lit(parallel, pl.Boolean)],\n            is_elementwise=True,\n        )\n</code></pre>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.overlap_coeff","title":"<code>overlap_coeff(other, substr_size=2, parallel=False)</code>","text":"<p>Treats substrings of size <code>substr_size</code> as a set. And computes the overlap coefficient as similarity between this word and the other. This is not the same as comparing bigrams.</p>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.overlap_coeff--parameters","title":"Parameters","text":"<p>other     If this is a string, then the entire column will be compared with this string. If this     is an expression, then perform element-wise jaccard similarity computation between this column     and the other (given by the expression). substr_size     The substring size for Jaccard similarity. E.g. if substr_size = 2, \"apple\" will be decomposed into     the set ('ap', 'pp', 'pl', 'le') before being compared. parallel     Whether to run the comparisons in parallel. Note that this is not always faster, especially     when used with other expressions or in group_by/over context.</p> Source code in <code>python/polars_ds/str_ext.py</code> <pre><code>def overlap_coeff(\n    self, other: Union[str, pl.Expr], substr_size: int = 2, parallel: bool = False\n) -&gt; pl.Expr:\n    \"\"\"\n    Treats substrings of size `substr_size` as a set. And computes the overlap coefficient as\n    similarity between this word and the other. This is not the same as comparing bigrams.\n\n    Parameters\n    ----------\n    other\n        If this is a string, then the entire column will be compared with this string. If this\n        is an expression, then perform element-wise jaccard similarity computation between this column\n        and the other (given by the expression).\n    substr_size\n        The substring size for Jaccard similarity. E.g. if substr_size = 2, \"apple\" will be decomposed into\n        the set ('ap', 'pp', 'pl', 'le') before being compared.\n    parallel\n        Whether to run the comparisons in parallel. Note that this is not always faster, especially\n        when used with other expressions or in group_by/over context.\n    \"\"\"\n    if isinstance(other, str):\n        other_ = pl.lit(other, pl.Utf8)\n    else:\n        other_ = other\n\n    return self._expr.register_plugin(\n        lib=_lib,\n        symbol=\"pl_overlap_coeff\",\n        args=[other_, pl.lit(substr_size, pl.UInt32), pl.lit(parallel, pl.Boolean)],\n        is_elementwise=True,\n    )\n</code></pre>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.snowball","title":"<code>snowball(no_stopwords=True, parallel=False)</code>","text":"<p>Applies the snowball stemmer for the column. The column is supposed to be a column of single words.</p>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.snowball--parameters","title":"Parameters","text":"<p>no_stopwords     If true, stopwords will be mapped to None. If false, stopwords will be stemmed. parallel     Whether to run the comparisons in parallel. Note that this is not always faster, especially     when used with other expressions or in group_by/over context.</p> Source code in <code>python/polars_ds/str_ext.py</code> <pre><code>def snowball(self, no_stopwords: bool = True, parallel: bool = False) -&gt; pl.Expr:\n    \"\"\"\n    Applies the snowball stemmer for the column. The column is supposed to be a column of single words.\n\n    Parameters\n    ----------\n    no_stopwords\n        If true, stopwords will be mapped to None. If false, stopwords will be stemmed.\n    parallel\n        Whether to run the comparisons in parallel. Note that this is not always faster, especially\n        when used with other expressions or in group_by/over context.\n    \"\"\"\n    return self._expr.register_plugin(\n        lib=_lib,\n        symbol=\"pl_snowball_stem\",\n        args=[pl.lit(no_stopwords, pl.Boolean), pl.lit(parallel, pl.Boolean)],\n        is_elementwise=True,\n    )\n</code></pre>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.sorensen_dice","title":"<code>sorensen_dice(other, substr_size=2, parallel=False)</code>","text":"<p>Treats substrings of size <code>substr_size</code> as a set. And computes the Sorensen-Dice similarity between this word and the other. This is not the same as comparing bigrams.</p>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.sorensen_dice--parameters","title":"Parameters","text":"<p>other     If this is a string, then the entire column will be compared with this string. If this     is an expression, then perform element-wise jaccard similarity computation between this column     and the other (given by the expression). substr_size     The substring size for Jaccard similarity. E.g. if substr_size = 2, \"apple\" will be decomposed into     the set ('ap', 'pp', 'pl', 'le') before being compared. parallel     Whether to run the comparisons in parallel. Note that this is not always faster, especially     when used with other expressions or in group_by/over context.</p> Source code in <code>python/polars_ds/str_ext.py</code> <pre><code>def sorensen_dice(\n    self, other: Union[str, pl.Expr], substr_size: int = 2, parallel: bool = False\n) -&gt; pl.Expr:\n    \"\"\"\n    Treats substrings of size `substr_size` as a set. And computes the Sorensen-Dice similarity between\n    this word and the other. This is not the same as comparing bigrams.\n\n    Parameters\n    ----------\n    other\n        If this is a string, then the entire column will be compared with this string. If this\n        is an expression, then perform element-wise jaccard similarity computation between this column\n        and the other (given by the expression).\n    substr_size\n        The substring size for Jaccard similarity. E.g. if substr_size = 2, \"apple\" will be decomposed into\n        the set ('ap', 'pp', 'pl', 'le') before being compared.\n    parallel\n        Whether to run the comparisons in parallel. Note that this is not always faster, especially\n        when used with other expressions or in group_by/over context.\n    \"\"\"\n    if isinstance(other, str):\n        other_ = pl.lit(other, dtype=pl.Utf8)\n    else:\n        other_ = other\n\n    return self._expr.register_plugin(\n        lib=_lib,\n        symbol=\"pl_sorensen_dice\",\n        args=[other_, pl.lit(substr_size, pl.UInt32), pl.lit(parallel, pl.Boolean)],\n        is_elementwise=True,\n    )\n</code></pre>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.str_jaccard","title":"<code>str_jaccard(other, substr_size=2, parallel=False)</code>","text":"<p>Treats substrings of size <code>substr_size</code> as a set. And computes the jaccard similarity between this word and the other. This is not the same as comparing bigrams.</p>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.str_jaccard--parameters","title":"Parameters","text":"<p>other     If this is a string, then the entire column will be compared with this string. If this     is an expression, then perform element-wise jaccard similarity computation between this column     and the other (given by the expression). substr_size     The substring size for Jaccard similarity. E.g. if substr_size = 2, \"apple\" will be decomposed into     the set ('ap', 'pp', 'pl', 'le') before being compared. parallel     Whether to run the comparisons in parallel. Note that this is not always faster, especially     when used with other expressions or in group_by/over context.</p> Source code in <code>python/polars_ds/str_ext.py</code> <pre><code>def str_jaccard(\n    self, other: Union[str, pl.Expr], substr_size: int = 2, parallel: bool = False\n) -&gt; pl.Expr:\n    \"\"\"\n    Treats substrings of size `substr_size` as a set. And computes the jaccard similarity between\n    this word and the other. This is not the same as comparing bigrams.\n\n    Parameters\n    ----------\n    other\n        If this is a string, then the entire column will be compared with this string. If this\n        is an expression, then perform element-wise jaccard similarity computation between this column\n        and the other (given by the expression).\n    substr_size\n        The substring size for Jaccard similarity. E.g. if substr_size = 2, \"apple\" will be decomposed into\n        the set ('ap', 'pp', 'pl', 'le') before being compared.\n    parallel\n        Whether to run the comparisons in parallel. Note that this is not always faster, especially\n        when used with other expressions or in group_by/over context.\n    \"\"\"\n    if isinstance(other, str):\n        other_ = pl.lit(other, dtype=pl.Utf8)\n    else:\n        other_ = other\n\n    return self._expr.register_plugin(\n        lib=_lib,\n        symbol=\"pl_str_jaccard\",\n        args=[other_, pl.lit(substr_size, pl.UInt32), pl.lit(parallel, pl.Boolean)],\n        is_elementwise=True,\n    )\n</code></pre>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.tokenize","title":"<code>tokenize(pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', stem=False)</code>","text":"<p>Tokenize the string according to the pattern. This will only extract the words satisfying the pattern.</p>"},{"location":"str_ext/#polars_ds.str_ext.StrExt.tokenize--parameters","title":"Parameters","text":"<p>pattern     The word pattern to extract stem     If true, then this will stem the words and keep only the unique ones. Stop words     will be removed. (Common words like <code>he</code>, <code>she</code>, etc., will be removed.)</p> Source code in <code>python/polars_ds/str_ext.py</code> <pre><code>def tokenize(self, pattern: str = r\"(?u)\\b\\w\\w+\\b\", stem: bool = False) -&gt; pl.Expr:\n    \"\"\"\n    Tokenize the string according to the pattern. This will only extract the words\n    satisfying the pattern.\n\n    Parameters\n    ----------\n    pattern\n        The word pattern to extract\n    stem\n        If true, then this will stem the words and keep only the unique ones. Stop words\n        will be removed. (Common words like `he`, `she`, etc., will be removed.)\n    \"\"\"\n    out = self._expr.str.extract_all(pattern)\n    if stem:\n        out = out.list.eval(\n            pl.element()\n            .register_plugin(\n                lib=_lib,\n                symbol=\"pl_snowball_stem\",\n                args=[pl.lit(True, pl.Boolean), pl.lit(False, pl.Boolean)],\n                is_elementwise=True,\n            )  # True to no stop word, False to Parallel\n            .drop_nulls()\n        )\n    return out\n</code></pre>"}]}